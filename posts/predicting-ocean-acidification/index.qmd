---
title: "Kaggle Competition Winning Code: Predicting Ocean Acidification"
description: "Ocean acidification, driven primarily by human-induced carbon emissions, already poses a huge problem for our oceans, economy, and well being. For proper ecosystem management, is important to understand which areas of the ocean are at higher risk. This workbook uses machine learning techniques to accurately predict ocean acidification to aid in management."
author:
  - name: Victoria Cutler
    url: https://victoriacutler.github.io # can also add orchid id under here
    affiliation: MEDS 
    affiliation-url: https://ucsb-meds.github.io
date: 2023-03-22
categories: [CAL COFI, OCEAN ACIDIFICATION, MACHINE LEARNING, EXTREME GRADIENT BOOSTING, R]
citation:
  url: https://victoriacutler.github.io/posts/predicting-ocean-acidification/
bibliography: references.bib
format:
  html:
    code-fold: false
    code-summary: "code dropdown"
image: "preview-image.png"
draft: false # "true" will mean this is a draft post so it wont show up on my site
---

## What is Ocean Acidification?

Our oceans absorb about 30% of the CO~2~ in our atmosphere. So, as CO~2~ emissions increase, CO~2~ in our oceans does, as well. This may not sound like a problem, but CO~2~ quickly breaks down and creates acid that disintegrates things like sea shells and coral reefs that fish and shellfish depend on.[@NOAA2021] Coral and shellfish are important not only for the health of the ecosystem, but also us. More than 3 billion people around the world rely on food from our oceans[@TNC2021], and ocean acidification poses a huge risk to the availability of the food.

## Kaggle Competition Background

The California Cooperative Oceanic Fisheries Investigations (CalCOFI for short) studies marine ecosystems across California to help drive sustainable environmental management in the context of climate change.[@CalCOFI2023] In March of 2023, CalCOFI partnered with the Bren School at UC Santa Barbara to search for an effective predictive model for ocean acidification. As part of my Environmental Data Science class, "Machine Learning in Environmental Science", us master's students had all entered the Kaggle competition to find the best predictive machine learning model given the data available[^1]. **The prize? CalCOFI swag** **(and helping sustainable ecosystem management).**

[^1]: The data and associated metadata can be found here: https://calcofi.org/data/oceanographic-data/bottle-database/

## The Winning Kaggle Competition Code \[Note: Still Being Cleaned-Up!\]

### Load Libraries

```{r warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(janitor)
library(ggplot2)
library(recipes)
library(xgboost)
library(beepr)
```

### Load and Clean Data

```{r warning=FALSE, message=FALSE}
# storing file paths
train_file_path <- here("posts","predicting-ocean-acidification","data", "train.csv")
test_file_path <- here("posts","predicting-ocean-acidification","data", "test.csv")

# reading in data
train_raw <- read_csv(file = train_file_path) |> 
  clean_names() |> 
  glimpse()

test_raw <- read_csv(file = test_file_path) |> 
  clean_names() |> 
  glimpse()

# looking at data frame similarities / differences
colnames_df1 <- colnames(train_raw)
colnames_df2 <- colnames(test_raw)
  # common columns
common_cols <- intersect(colnames_df1, colnames_df2)
common_cols

  # different columns
unique_cols_df1 <- setdiff(colnames_df1, common_cols)
unique_cols_df1

unique_cols_df2 <- setdiff(colnames_df2, common_cols)
unique_cols_df2

# adjusting training data per column naming differences
train_data <- train_raw |> 
  select(-x13) |> # all NULL
  rename(ta1 = ta1_x) # to match the test_raw naming
```

### Data Exploration

```{r}
# adding all the data together to plot data distributions for variables to check for variance / outliers to inform which ML algorithm to use
full_data <- train_data |> 
  select(-dic) |> 
  rbind(test_raw)

# histograms to explore data distributions and look at variance/outliers
ggplot(data = full_data, aes(x = no2u_m)) +
  geom_histogram()

ggplot(data = full_data, aes(x = no3u_m)) +
  geom_histogram()

  # potential outliers here:
ggplot(data = full_data, aes(x = nh3u_m)) +
  geom_histogram()

ggplot(data = full_data, aes(x = r_temp)) +
  geom_histogram()

  # potential outliers here:
ggplot(data = full_data, aes(x = r_depth)) +
  geom_histogram()

ggplot(data = full_data, aes(x = r_sal)) +
  geom_histogram()

  # potential outliers here:
ggplot(data = full_data, aes(x = r_dynht)) +
  geom_histogram()

  # potential outliers here:
ggplot(data = full_data, aes(x = r_nuts)) +
  geom_histogram()
```

### XGBoost Model

#### Recipe

```{r, eval = FALSE}
# using the xgboost model since this is a powerful ML algorithm that can handle outliers. Code is adapted from: https://bradleyboehmke.github.io/HOML/gbm.html

  # recipe
xgb_prep <- recipe(dic ~ ., data = train_data) %>%
  prep(training = train_data, retain = TRUE) %>%
  juice()

  # separating data used for predictions from the predicted values
X <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "dic")])
Y <- xgb_prep$dic
```

#### Hyperparameter Tuning and Model Fit Assessment

```{r, eval = FALSE}
# set seed for reproducibility 
set.seed(123)

# creating a hyperparameter grid for all xgboost hyperparameters
hyper_grid <- expand.grid(
  eta = 0.01,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = 0.5, 
  colsample_bytree = 0.5,
  gamma = c(0, 1, 10, 100, 1000),
  lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  rmse = 0,          # a place to dump RMSE results
  trees = 0          # a place to dump required number of trees
)

# loop to search through the grid and apply hyperparameters to all 10 cv folds
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 4000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i],
      colsample_bytree = hyper_grid$colsample_bytree[i],
      gamma = hyper_grid$gamma[i], 
      lambda = hyper_grid$lambda[i], 
      alpha = hyper_grid$alpha[i]
    ) 
  )
  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)
  hyper_grid$trees[i] <- m$best_iteration
}

# store results where rmse is greater than 0
rmse_hp_results <- hyper_grid %>%
  filter(rmse > 0) %>%
  arrange(rmse)

# store the best hyperparameters
best_eta <- rmse_hp_results$eta[1]
best_md <- rmse_hp_results$max_depth[1]
best_cw <- rmse_hp_results$min_child_weight[1]
best_ss <- rmse_hp_results$subsample[1]
best_csbt <- rmse_hp_results$colsample_bytree[1]
best_g <- rmse_hp_results$gamma[1]
best_l <- rmse_hp_results$lambda[1]
best_a <- rmse_hp_results$alpha[1]

# beep when process is done
beep()
```

#### Training the Training Data Based on Optimal Hyperparameters

```{r, eval = FALSE}
# optimal parameter list
params <- list(
  eta = best_eta,
  max_depth = best_md,
  min_child_weight = best_cw,
  subsample = best_ss,
  colsample_bytree = best_csbt,
  gamma = best_g,
  lambda = best_l,
  alpha = best_a
)

# train the model on the training data given these optimal parameters
xgb.fit.final <- xgboost(
  params = params,
  data = X,
  label = Y,
  nrounds = 3944,
  objective = "reg:squarederror",
  verbose = 0
)
```

#### Predicting the Test Data Set DIC

```{r, eval = FALSE}
# predict the test data set
X_test <- as.matrix(test_raw)
y_pred <- predict(xgb.fit.final, newdata = X_test)
test_data_with_pred <- cbind(test_raw, DIC = y_pred) |> 
  select(id, DIC) # these are the only columns needed for kaggle competition
```

#### Writing the Final File to a CSV

```{r, eval = FALSE}
# prompt the user to choose their own file name to write to a csv
file_name <- file.choose(new = TRUE)

# write test data predictions to a file
write.csv(test_data_with_pred, file = file_name, row.names = FALSE)
```
