---
title: "Predicting Ocean Acidification for Ecosystem Management"
description: "Ocean acidification, driven primarily by human-induced carbon emissions, already poses a huge problem for our oceans, economy, and well being. For proper ecosystem management, is important to understand which areas of the ocean are at higher risk. This workbook uses machine learning techniques to accurately predict ocean acidification to aid in management."
author:
  - name: Victoria Cutler
    url: https://victoriacutler.github.io # can also add orchid id under here
    affiliation: MEDS 
    affiliation-url: https://ucsb-meds.github.io
date: 2023-03-22
categories: [CAL COFI, OCEAN ACIDIFICATION, MACHINE LEARNING, EXTREME GRADIENT BOOSTING, R]
citation:
  url: https://victoriacutler.github.io/posts/predicting-ocean-acidification/
bibliography: references.bib
format:
  html:
    code-fold: false
    code-summary: "code dropdown"
image: "preview-image.png"
draft: false # "true" will mean this is a draft post so it wont show up on my site
---

## What is Ocean Acidification?

Our oceans absorb about 30% of the CO~2~ in our atmosphere. So, as CO~2~ emissions increase, CO~2~ in our oceans does, as well. This may not sound like a problem, but CO~2~ quickly breaks down and creates acid that disintegrates things like sea shells and coral reefs that fish, shellfish, and ultimately, humans, depend on. [@NOAA2021] More than 3 billion people around the world rely on food from our oceans[@TNC2021], and ocean acidification poses a huge risk to the availability of the food.

## Kaggle Competition Background

The California Cooperative Oceanic Fisheries Investigations (CalCOFI for short) studies marine ecosystems across California to help drive sustainable environmental management in the context of climate change.[@CalCOFI2023] In March of 2023, CalCOFI partnered with the Bren School at UC Santa Barbara to search for an effective predictive model for ocean acidification. As part of my Environmental Data Science class, "Machine Learning in Environmental Science", us master's students had all entered the Kaggle competition to find the best predictive machine learning model given the data available[^1]. **The prize? CalCOFI swag** **(and helping sustainable ecosystem management).**

[^1]: The data and associated metadata can be found here: https://calcofi.org/data/oceanographic-data/bottle-database/

## The Winning Competition Code

Yes, I won. Don't be so shocked. And yes, I got a lot of swag! Please read on, below, for not just the code but also my thought process a long the way.

![See above for the final Kaggle Competition leaderboard results](./leaderboard.png)

### Load Libraries

```{r warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(janitor)
library(ggplot2)
library(recipes)
library(xgboost)
library(beepr)
```

### Load and Clean Data

Here I load in the training and testing data (already pre-split to ensure consistency in competition assessment). I then correct any data quality issues. It turned out that the training and testing data had a couple inconsistencies, so I fixed those up and removed any unnecessary columns.

```{r warning=FALSE, message=FALSE}
# storing file paths
train_file_path <- here("posts","predicting-ocean-acidification","data", "train.csv")
test_file_path <- here("posts","predicting-ocean-acidification","data", "test.csv")

# reading in data

  # raw training data
train_raw <- read_csv(file = train_file_path) |> 
  clean_names() |> 
  glimpse()

  # raw testing data
test_raw <- read_csv(file = test_file_path) |> 
  clean_names() |> 
  glimpse()

# looking at data frame similarities / differences
colnames_df1 <- colnames(train_raw)
colnames_df2 <- colnames(test_raw)
  # common columns
common_cols <- intersect(colnames_df1, colnames_df2)
common_cols

  # looking for different columns between the training and testing data
unique_cols_df1 <- setdiff(colnames_df1, common_cols)
unique_cols_df1

unique_cols_df2 <- setdiff(colnames_df2, common_cols)
unique_cols_df2

# adjusting training data per column naming differences
train_data <- train_raw |> 
  select(-x13) |> # removing since all NULL
  rename(ta1 = ta1_x) # changing the name to match the test_raw naming
```

### Data Exploration for Machine Learning Algorithm Selection

Below I create a full data set with both the training and testing data. I then explore data distributions for each of the features since this will inform the machine learning algorithm I select for ocean acidification prediction.

```{r}
# adding all the data together to plot data distributions for each variable. This is to check variance and outliers to inform which ML algorithm to use
full_data <- train_data |> 
  select(-dic) |> # removing the dic variable we're predicting since the test data doesn't have this variable and i don't want to look at the data distribution in case leaving out the test data introduces bias
  rbind(test_raw)

# histograms to explore data distributions and look at variance/outliers
ggplot(data = full_data, aes(x = no2u_m)) +
  geom_histogram()

ggplot(data = full_data, aes(x = no3u_m)) +
  geom_histogram()

ggplot(data = full_data, aes(x = nh3u_m)) +
  geom_histogram() # potential outliers here

ggplot(data = full_data, aes(x = r_temp)) +
  geom_histogram()

ggplot(data = full_data, aes(x = r_depth)) +
  geom_histogram() # potential outliers here:

ggplot(data = full_data, aes(x = r_sal)) +
  geom_histogram()

ggplot(data = full_data, aes(x = r_dynht)) +
  geom_histogram() # potential outliers here

ggplot(data = full_data, aes(x = r_nuts)) +
  geom_histogram() # potential outliers here
```

### XGBoost Model

After plotting the data distributions, I see a lot of outliers. I also know that I can't use a classification algorithm since my prediction variable is continuous. I choose to build an extreme gradient boosted treet (XGBoost) machine learning algorithm to build my ocean acidification prediction model. I use this algorithm since gradient boosted models, if properly tuned, can be the most flexible and accurate predictive models. I choose an extreme gradient boosted model since it can handle outliers well and has many hyperparameters that reduce overfitting.

#### Recipe

```{r, eval = FALSE}
# using the xgboost model since this is a powerful ML algorithm that can handle outliers. Code is adapted from: https://bradleyboehmke.github.io/HOML/gbm.html

  # recipe
xgb_prep <- recipe(dic ~ ., data = train_data) %>% # using all features for predicting dic (ocean acidification)
  prep(training = train_data, retain = TRUE) %>%
  juice()

  # separating data used for predictions from the predicted values
X <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "dic")])
Y <- xgb_prep$dic
```

#### Hyperparameter Tuning and Model Fit Assessment

```{r, eval = FALSE}
# set seed for reproducibility 
set.seed(123)

# creating a hyperparameter grid for all xgboost hyperparameters
hyper_grid <- expand.grid(
  eta = 0.01,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = 0.5, 
  colsample_bytree = 0.5,
  gamma = c(0, 1, 10, 100, 1000),
  lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
  rmse = 0,          # a place to dump RMSE results
  trees = 0          # a place to dump required number of trees
)

# loop to search through the grid and apply hyperparameters to all 10 cv folds
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 4000,
    objective = "reg:squarederror",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i],
      colsample_bytree = hyper_grid$colsample_bytree[i],
      gamma = hyper_grid$gamma[i], 
      lambda = hyper_grid$lambda[i], 
      alpha = hyper_grid$alpha[i]
    ) 
  )
  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)
  hyper_grid$trees[i] <- m$best_iteration
}

# store results where rmse is greater than 0
rmse_hp_results <- hyper_grid %>%
  filter(rmse > 0) %>%
  arrange(rmse)

# store the best hyperparameters
best_eta <- rmse_hp_results$eta[1]
best_md <- rmse_hp_results$max_depth[1]
best_cw <- rmse_hp_results$min_child_weight[1]
best_ss <- rmse_hp_results$subsample[1]
best_csbt <- rmse_hp_results$colsample_bytree[1]
best_g <- rmse_hp_results$gamma[1]
best_l <- rmse_hp_results$lambda[1]
best_a <- rmse_hp_results$alpha[1]

# beep when process is done since this process can take hours
beep()
```

#### Training the Training Data Based on Optimal Hyperparameters

```{r, eval = FALSE}
# optimal parameter list
params <- list(
  eta = best_eta,
  max_depth = best_md,
  min_child_weight = best_cw,
  subsample = best_ss,
  colsample_bytree = best_csbt,
  gamma = best_g,
  lambda = best_l,
  alpha = best_a
)

# train the model on the training data given these optimal parameters
xgb.fit.final <- xgboost(
  params = params,
  data = X,
  label = Y,
  nrounds = 3944,
  objective = "reg:squarederror",
  verbose = 0
)
```

#### Predicting the Test Data Set Ocean Acidification for Competition Assessment 

```{r, eval = FALSE}
# predict on the test data set
X_test <- as.matrix(test_raw)
y_pred <- predict(xgb.fit.final, newdata = X_test)

test_data_with_pred <- cbind(test_raw, DIC = y_pred) |> 
  select(id, DIC) # these are the only columns needed for kaggle competition
```

#### Writing the Final File to a CSV for Competition Assessment

```{r, eval = FALSE}
# prompt the user to choose their own file name to write to a csv
file_name <- file.choose(new = TRUE)

# write test data predictions to a file
write.csv(test_data_with_pred, file = file_name, row.names = FALSE)
```

#### Final Thoughts

Before the Kaggle Competition closed, each "test data" submission (aka each submission of ocean acidification predictions) were automatically assessed against the *real* ocean acidification values. Each submission was then given a root mean squared error (RMSE) score based on performance, and everyone could see on the leaderboard how their model performance stacked up compared to others' model performance. I noticed that many people submitted many times, re-training and fine tuning their models to get better performance.

However, the RMSE score was calculated only using 70% of the submitted data. This meant that the other 30% of the data were left out of RMSE performance metrics prior to the competition end date. As a result, every time a classmate re-trained a model to obtain better performance, they were essentially over fitting to that 70% of the training data. My hunch is that my model won because the 30% of data withheld were not representative of the patterns found in the rest of the data, and I made sure to not introduce bias by re-training and re-tuning my model to serve that 70%.
