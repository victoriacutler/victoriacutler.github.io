[
  {
    "objectID": "posts/solar-equity/index.html",
    "href": "posts/solar-equity/index.html",
    "title": "Is Rooftop Solar Equitable?",
    "section": "",
    "text": "Objective\nIn this post, I explore if income is a statistically significant predictor in rooftop solar adoption in California. I also look at overall solar saturation2 as an informal gauge of the potential growth and increased severity of this issue.\n\n\nMethods\nSince disaggregated household data for income and rooftop solar is unavailable for public use, I use:\n\nIncome data on a census tract basis found here from the Federal Financial Institutions Examination Council (FFIEC) which maintains income-related census data available for download;\nRooftop solar data found here from Google Project Sunroof which uses Google overhead imagery, weather station data, and machine learning algorithms to estimate rooftop solar potential of US buildings down to the census tract level; and\nCalifornia census tract spatial data found here for mapping and visualizing at the census tract level.\n\nAfter reading in the data, I clean the data, select only the columns of interest, and create additional columns needed for joining of data and performing analyses. Most notably, I created a variable called pct_qual_with_solar. This variable ranges from 0% to 100% and represents the ratio of number of buildings with solar with respect to the number of buildings deemed as “qualified” for rooftop solar3. As an example, if census tract A has 10 buildings qualified for rooftop solar, and 4 buildings in census tract A already have rooftop solar, this census tract would have a pct_qual_with_solar of 40%. I refer to this variable as “solar saturation” since it reveals how saturated a census tract is in terms of rooftop solar adoption. The denominator in this variable is qualified homes as opposed to total buildings, since I feel the latter could lead to bias if income is correlated with locations with census tracts of low rooftop solar qualifications (such as insufficient roof size).\n\n\n\nI first look at the distribution of the solar saturation for California. We see right skew which hints that these data may be a good candidate for log-normalization. However, after taking the log of my data and performing an Ordinary Least Squares (OSL) regression on the log-normalized data, I noticed that the assumptions for OLS were not drastically better met. Additionally, due to the large amount of data we have at 0% solar saturation, to perform a log-normalized OLS regression, this would require either data manipulation, or removing much of the data.\n\n\n\n\n\nIn running my regression, I determine it makes the most sense to account for sunlight when looking at the relationship between census tract income and census tract solar generation. This is because, similar to how solar-qualified buildings may correlate with income, sunlight may also correlate with income, and we want to ensure we are mitigating any omitted variables bias.\nI therefore regress as follows:\n\\[pctsolarsat_i =\\beta_{0}+\\beta_{1} \\cdot sunlight_i +\\beta_{2} \\cdot \\text income_i+\\varepsilon_i\\]\nWhile I considered that there may be an interaction effect between income and sunlight, I found that variance in solar adoption was not largely explained by this interaction.\nRunning this regression effectively performs a hypothesis test for us where I have:\n- The Null Hypothesis: There is no relationship between census tract median income and solar saturation.\n- The Alternative Hypothesis: There is a relationship between census tract median income and solar saturation.\n\n\nResults and Discussion\n\nLinear Regression & Hypothesis Testing\nAfter regressing, I end up with the following model:\n\\[solarsaturation =-0.68 + 0.0000080 \\cdot sunlight + 0.0000384 \\cdot income\\]\n\n\n\n\nTbl 1. Linear Model Results for Below Predictors\n\n \nSolar Saturation (%)\n\n\nPredictors\nEstimates\nConf. Int (95%)\nP-value\n\n\nIntercept\n-0.6756526\n-0.8449613 – -0.5063439\n<0.001\n\n\nMedian Income\n0.0000384\n0.0000369 – 0.0000399\n<0.001\n\n\nAverage Yearly Sunlight Generation (kWh)\n0.0000080\n0.0000055 – 0.0000106\n<0.001\n\n\nObservations\n6379\n\n\nR2 / R2 adjusted\n0.293 / 0.292\n\n\n\n\n\n\nWe see from this model output that both median income and sunlight have a positive effect on solar saturation in a census tract, which is expected. This effect is statistically significant (p < 0.001) and we can therefore reject the null hypothesis that census tract income has no relationship with solar saturation.\nSince our coefficient for median income is 0.0000384, for every $1 increase in median yearly income, our model predicts a 0.0000384% increase in solar saturation for that census tract. To put this in more real world terms, for every $26,042 increase in yearly median census tract income, this model predicts a 1% increase in solar saturation.\nIn separate calculations, I also find that the mean solar saturation in higher income census tracts is 4.85%, compared to merely 1.69% in lower income census tracts.\n\n\n\n\n\nSpatial Analysis\nTo visualize this difference in census tract solar adoption, I map solar saturation by census tract for both higher income census tracts and lower income census tracts. You can see, below, visually, that there are much higher saturation levels (represented by the darker blues) in the higher income census tracts. You can also see, evidenced by the amount of grey, that there is missing data for a large number of census tracts. This is due to missing solar data for about 557 of the approximately 8,057 census tracts in California. This could lead to bias in the model, especially if this missing data is not due to random factors.\nAnother take away from this visual is how low solar saturation percentages are overall. This is good and bad news. It means there is still a lot of potential for rooftop solar growth, which is great for clean energy growth. However, this also drives home how important it is to ensure equity in the rooftop solar industry, since these inequities may escalate as solar adoption continues to increase.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpportunities for Improvement\nWhile the findings of this study are statistically significant, this model may still be experiencing sources of bias. Below I outline those potential biases, and how further research could aid in better model development:\n\nMissing solar data\nAs mentioned above, the solar data sourced from Google Project Sunroof is missing data from 557 census tracts in California. It is therefore possible that this missing data is creating bias in our model. To ensure that this is not the case, further research could be done to (1) obtain data from these census tracts or (2) inquire to Google Project Sunroof to understand their methodology and gauge whether or not this would lead to an unrepresentative sample.\n\n\n\nInclusion of non-residential buildings\nGoogle Project Sunroof looks at building solar potential for all buildings, not just residential buildings. If non-residential buildings and associated solar adoption is not randomly distributed across census tracts, this could lead to bias in this model. For better modeling, Google Project Sunroof developers could distinguish non-residential and residential buildings.\nMissing variables\nWith any model, there is a chance of omitted variables bias. For further research, I suggest looking into other potential variables that may be correlated with income. Including demographic information such as race could be a good place to start.\n\n\n\nConclusion\nThis statistical analysis provides evidence that low income households have statistically significantly lower levels of rooftop solar adoption. We also see that solar saturation percentages are quite low, which suggests that the rooftop solar industry has the potential to grow much larger. This means that it is of utmost importance that rooftop solar policy is equitable. The California Public Utilities Commission is likely to approve policy to lower financial incentives to rooftop solar adoption due, in large part, to equity concerns. I hope, however, to see policy that encourages rooftop solar but also ensures equity.\n\n\n\n\n\n\nReferences\n\nPenn, Ivan. 2022. “California Regulators Propose Cutting Compensation for Rooftop Solar.” November 10, 2022. https://www.nytimes.com/2022/11/10/business/energy-environment/california-rooftop-solar-net-metering.html.\n\nFootnotes\n\n\nLow income households often lack access to solar since (1) financing options are often needed to cover the high upfront cost of solar (2) if English isn’t a first language, or if multiple jobs are worked, it may be a struggle to find the time to research solar, and (3) renters don’t have the autonomy to install solar panels, and landlords do not have an incentive to help their tenants save on their electric bill.↩︎\nFor a definition of “solar saturation” please refer to the “Methods” section, below, for a definition and example.↩︎\nPer Google Project Sunroof criteria which considers amount of sunlight and roof space.↩︎\n\nCitationBibTeX citation:@online{cutler2022,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Is {Rooftop} {Solar} {Equitable?}},\n  date = {2022-12-04},\n  url = {https://victoriacutler.github.io/postxs/solar-equity/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2022. “Is Rooftop Solar Equitable?”\nDecember 4, 2022. https://victoriacutler.github.io/postxs/solar-equity/."
  },
  {
    "objectID": "posts/remote-sensing/index.html",
    "href": "posts/remote-sensing/index.html",
    "title": "Remote Sensing for Environmental Issues",
    "section": "",
    "text": "Footnotes\n\n\nForked from Dr. Samantha Stevenson: https://github.com/victoriacutler/workingwithenvironmentaldatasets-remotesensing↩︎\n\nCitationBibTeX citation:@online{cutler2023,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Remote {Sensing} for {Environmental} {Issues}},\n  date = {2023-07-09},\n  url = {https://victoriacutler.github.io/posts/remote-sensing/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2023. “Remote Sensing for Environmental\nIssues.” July 9, 2023. https://victoriacutler.github.io/posts/remote-sensing/."
  },
  {
    "objectID": "posts/2022-12-07-AI/index.html",
    "href": "posts/2022-12-07-AI/index.html",
    "title": "Ethical AI for Just Decision Making",
    "section": "",
    "text": "The judiciary process is set on top of the premise that in-person-judgement in the courtroom helps in accurate decision making. But humans are filled with bias. And what’s more, humans can also be very good at deceiving (Gladwell 2019). Does this mean, then, that we should be using AI to determine what we should do when it comes to the judgement and placement of people? Or, for that matter, for any decisions that may affect humanity?\nUnfortunately, there are countless examples in which AI algorithms create inequities. For one, while at graduate school, Joy Buolamwini sought to create a mirror that projects various “heroes” on her own face. In doing so, she uncovered something alarming. The mirror wouldn’t recognize her own face unless she placed a white mask over it. She then examined the facial recognition technology at Amazon, Google, Microsoft, and IBM and found that error rates within this tech for white men were at less than 1%, yet for black and brown females, this error rate was at a whopping 30%. Buolamwini continued her studies to find that this difference in error rates was largely due to the fact that the data set of imagery was mostly white men (Shacknai 2022). And this is no shock given that performing a quick Google image search of “men”, we largely see white men.\n“When we think about transitioning into the world of tech, the same things that are being marginalized and ignored by the conversations we have around racial inequality in the U.S.—skin tone and colorism—are also being marginalized and ignored in the tech world” - Harvard University Professor Dr. Monk1\nSimilarly, predictive policing2 makes use of past policing data to predict which neighborhoods to police next. While this methodology may be seemingly “unbiased” since it circumvents biased people in the decision making process, the reality is just the opposite. This is due to the fact that policing data is systemically biased and racist. As a result, predictive policing tools can create vicious feedback loops in which neighborhoods that were in the past more frequently policed due to racism and biases are now more policed, which inevitably leads to more arrests/infractions, and therefore more data for predictive policing models to point even more to those very same neighborhoods (Lum and Isaac 2016).\nThis is because “raw data” is not, in fact, raw, in the sense that it is unbiased. It is grounded in the inequities present at the time. For that reason, if we are to use AI to help us with decision making, it is imperative that we re-contextualize our data to recognize the biases at play to help that our model-inputs are no longer biased (Gitelman 2013).\nBut how can we ensure our raw data is re-contextualized and bias-free? One possible mitigation strategy is to utilize datasheets for datasets for any data used in our models (Gebru et al. 2018). In “Datasheets for Datasets” by Timnit Gebru, Gebru brings to light how if we provide a plethora of information about every dataset, we can help avoid unintended consequences in the use of that data. “Datasheets for Datasets” provides a plethora of potential provocative questions to provide about our datasets, such as:\n\n“For what purpose was the dataset created?”\n“Who funded the creation of the dataset?”\n“Does the dataset contain all possible instances or is it a sample (not necessarily random of instances) from a larger set?”\n“Is any information missing from individual instances?”\n“Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?”\n\nWhen we have datasheets comprehensively describing our data, we are able to more accurately identify biases for AI computer model building.\nAnd to that point, our AI models themselves should also come along with datasheets for increased transparency, better model comprehension, and ultimately, better model quality checks. These more transparent AI models are often referred to as “white box” models, which are so transparent such that any outside observer can clearly see how automated AI models arrive at the decision. In this way, it would be much easier to see if the model is making recommendations rooted in bias. “Black box” models do not have this sort of transparency and thus it is nearly impossible to see if the model is using incorrect or unjust assumptions as part of it’s methodology (McNally, n.d.). When we think of how AI models have the ability to morph as the data that the model is continuously trained on comes in, white box models have even more weight. This is because the method that a model was once using and was once understood, may be drastically different from the method used today. For this reason, it is greatly important to perform consistent checks on model assumptions.\nSo - back to the original question: should AI be used for decision making?\nAll in all, AI will be as unjust as the “unjustness” that goes in. But with correct input data and model checks and balances, it is certainly possible to create AI algorithms that are at least less biased than the average human. Algorithms may never be perfect, but that is exactly why it is so important for our models and input data to be transparent and open for diverse sets of stakeholders to scrutinize.\n\n\n\n\n\nReferences\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé, and Kate Crawford. 2018. “Datasheets for Datasets.” https://doi.org/10.48550/ARXIV.1803.09010.\n\n\nGitelman, Lisa, ed. 2013. \"Raw Data\" Is an Oxymoron. The MIT Press. https://doi.org/10.7551/mitpress/9302.001.0001.\n\n\nGladwell, Malcolm. 2019. Talking to Strangers.\n\n\nLum, Kristian, and William Isaac. 2016. “To Predict and Serve?” Significance 13 (5): 14–19. https://doi.org/10.1111/j.1740-9713.2016.00960.x.\n\n\nMcNally, Angelus. n.d. “Creating Trustworthy AI for the Environment: Transparency, Bias, and Beneficial Use.” https://www.scu.edu/environmental-ethics/resources/creating-trustworthy-ai-for-the-environment-transparency-bias-and-beneficial-use/.\n\n\nShacknai, Gabby. 2022. “Beauty in the Eye of the a.i.: How Inherent Racial Bias Has Shaped a.i. And What Brands Are Doing to Address It,” November. https://fortune.com/2022/11/22/tech-forward-everyday-ai-beauty-bias/.\n\nFootnotes\n\n\n(Shacknai 2022)↩︎\nPredictive policing is the use of computer algorithms to predict which neighborhoods to police.↩︎\n\nCitationBibTeX citation:@online{cutler2022,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Ethical {AI} for {Just} {Decision} {Making}},\n  date = {2022-12-07},\n  url = {https://victoriacutler.github.io/posts/2022-10-24-url-title/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2022. “Ethical AI for Just Decision\nMaking.” December 7, 2022. https://victoriacutler.github.io/posts/2022-10-24-url-title/."
  },
  {
    "objectID": "posts/predicting-ocean-acidification/index.html",
    "href": "posts/predicting-ocean-acidification/index.html",
    "title": "Predicting Ocean Acidification for Ecosystem Management",
    "section": "",
    "text": "Ocean acidification, driven primarily by human-induced carbon emissions, already poses a huge problem for our oceans, economy, and well being. For proper ecosystem management, is important to understand which areas of the ocean are at higher risk. This workbook uses machine learning techniques to accurately predict ocean acidification to aid in management."
  },
  {
    "objectID": "posts/predicting-ocean-acidification/index.html#what-is-ocean-acidification",
    "href": "posts/predicting-ocean-acidification/index.html#what-is-ocean-acidification",
    "title": "Predicting Ocean Acidification for Ecosystem Management",
    "section": "What is Ocean Acidification?",
    "text": "What is Ocean Acidification?\nOur oceans absorb about 30% of the CO2 in our atmosphere. So, as CO2 emissions increase, CO2 in our oceans does, as well. This may not sound like a problem, but CO2 quickly breaks down and creates acid that disintegrates things like sea shells and coral reefs that fish, shellfish, and ultimately, humans, depend on. (“What Is Ocean Acidification?” 2023) More than 3 billion people around the world rely on food from our oceans(“A Healthy Ocean Depends on Sustainably Managed Fisheries” 2021), and ocean acidification poses a huge risk to the availability of the food."
  },
  {
    "objectID": "posts/predicting-ocean-acidification/index.html#kaggle-competition-background",
    "href": "posts/predicting-ocean-acidification/index.html#kaggle-competition-background",
    "title": "Predicting Ocean Acidification for Ecosystem Management",
    "section": "Kaggle Competition Background",
    "text": "Kaggle Competition Background\nThe California Cooperative Oceanic Fisheries Investigations (CalCOFI for short) studies marine ecosystems across California to help drive sustainable environmental management in the context of climate change.(“About CalCOFI,” n.d.) In March of 2023, CalCOFI partnered with the Bren School at UC Santa Barbara to search for an effective predictive model for ocean acidification. As part of my Environmental Data Science class, “Machine Learning in Environmental Science”, us master’s students had all entered the Kaggle competition to find the best predictive machine learning model given the data available1. The prize? CalCOFI swag (and helping sustainable ecosystem management)."
  },
  {
    "objectID": "posts/predicting-ocean-acidification/index.html#the-winning-competition-code",
    "href": "posts/predicting-ocean-acidification/index.html#the-winning-competition-code",
    "title": "Predicting Ocean Acidification for Ecosystem Management",
    "section": "The Winning Competition Code",
    "text": "The Winning Competition Code\nYes, I won. Don’t be so shocked. And yes, I got a lot of swag! Please read on, below, for not just the code but also my thought process a long the way.\n\n\n\nSee above for the final Kaggle Competition leaderboard results\n\n\n\nLoad Libraries\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggplot2)\nlibrary(recipes)\nlibrary(xgboost)\nlibrary(beepr)\n\n\n\nLoad and Clean Data\nHere I load in the training and testing data (already pre-split to ensure consistency in competition assessment). I then correct any data quality issues. It turned out that the training and testing data had a couple inconsistencies, so I fixed those up and removed any unnecessary columns.\n\n# storing file paths\ntrain_file_path <- here(\"posts\",\"predicting-ocean-acidification\",\"data\", \"train.csv\")\ntest_file_path <- here(\"posts\",\"predicting-ocean-acidification\",\"data\", \"test.csv\")\n\n# reading in data\n\n  # raw training data\ntrain_raw <- read_csv(file = train_file_path) |> \n  clean_names() |> \n  glimpse()\n\nRows: 1,454\nColumns: 19\n$ id                <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ lat_dec           <dbl> 34.38503, 31.41833, 34.38503, 33.48258, 31.41432, 32…\n$ lon_dec           <dbl> -120.6655, -121.9983, -120.6655, -122.5331, -121.997…\n$ no2u_m            <dbl> 0.030, 0.000, 0.180, 0.013, 0.000, 0.080, 0.007, 0.0…\n$ no3u_m            <dbl> 33.80, 34.70, 14.20, 29.67, 33.10, 5.00, 38.76, 0.10…\n$ nh3u_m            <dbl> 0.00, 0.00, 0.00, 0.01, 0.05, 0.14, 0.00, 0.05, 0.03…\n$ r_temp            <dbl> 7.79, 7.12, 11.68, 8.33, 7.53, 13.05, 6.31, 19.28, 7…\n$ r_depth           <dbl> 323, 323, 50, 232, 323, 50, 518, 2, 323, 170, 30, 23…\n$ r_sal             <dbl> 141.2, 140.8, 246.8, 158.5, 143.4, 293.1, 114.6, 403…\n$ r_dynht           <dbl> 0.642, 0.767, 0.144, 0.562, 0.740, 0.176, 0.857, 0.0…\n$ r_nuts            <dbl> 0.00, 0.00, 0.00, 0.01, 0.05, 0.14, 0.00, 0.05, 0.03…\n$ r_oxy_micromol_kg <dbl> 37.40948, 64.81441, 180.29150, 89.62595, 60.03062, 2…\n$ x13               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ po4u_m            <dbl> 2.77, 2.57, 1.29, 2.27, 2.53, 0.75, 3.16, 0.19, 2.63…\n$ si_o3u_m          <dbl> 53.86, 52.50, 13.01, 38.98, 49.28, 5.90, 76.82, 0.17…\n$ ta1_x             <dbl> 2287.45, 2279.10, 2230.80, 2265.85, 2278.49, 2219.87…\n$ salinity1         <dbl> 34.1980, 34.0740, 33.5370, 34.0480, 34.1170, 33.2479…\n$ temperature_deg_c <dbl> 7.82, 7.15, 11.68, 8.36, 7.57, 13.06, 6.36, 19.28, 7…\n$ dic               <dbl> 2270.170, 2254.100, 2111.040, 2223.410, 2252.620, 20…\n\n  # raw testing data\ntest_raw <- read_csv(file = test_file_path) |> \n  clean_names() |> \n  glimpse()\n\nRows: 485\nColumns: 17\n$ id                <dbl> 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463…\n$ lat_dec           <dbl> 34.32167, 34.27500, 34.27500, 33.82833, 33.82833, 33…\n$ lon_dec           <dbl> -120.8117, -120.0333, -120.0333, -118.6250, -118.625…\n$ no2u_m            <dbl> 0.02, 0.00, 0.00, 0.00, 0.02, 0.17, 0.00, 0.00, 0.00…\n$ no3u_m            <dbl> 24.0, 25.1, 31.9, 0.0, 19.7, 4.2, 21.5, 26.1, 31.9, …\n$ nh3u_m            <dbl> 0.41, 0.00, 0.00, 0.20, 0.00, 0.10, 0.00, 0.01, 0.00…\n$ r_temp            <dbl> 9.51, 9.84, 6.60, 19.21, 10.65, 12.99, 10.29, 9.50, …\n$ r_depth           <dbl> 101, 102, 514, 1, 100, 19, 99, 169, 319, 523, 101, 5…\n$ r_sal             <dbl> 189.9, 185.2, 124.1, 408.1, 215.5, 284.9, 207.3, 173…\n$ r_dynht           <dbl> 0.258, 0.264, 0.874, 0.004, 0.274, 0.075, 0.266, 0.3…\n$ r_nuts            <dbl> 0.41, 0.00, 0.00, 0.20, 0.00, 0.10, 0.00, 0.01, 0.00…\n$ r_oxy_micromol_kg <dbl> 138.838300, 102.709200, 2.174548, 258.674300, 145.83…\n$ po4u_m            <dbl> 1.85, 2.06, 3.40, 0.27, 1.64, 0.65, 1.77, 2.17, 2.78…\n$ si_o3u_m          <dbl> 25.5, 28.3, 88.1, 2.5, 19.4, 5.4, 21.8, 31.5, 48.6, …\n$ ta1               <dbl> 2244.94, 2253.27, 2316.95, 2240.49, 2238.30, 2224.99…\n$ salinity1         <dbl> 33.8300, 33.9630, 34.2410, 33.4650, 33.7200, 33.3310…\n$ temperature_deg_c <dbl> 9.52, 9.85, 6.65, 19.21, 10.66, 12.99, 10.30, 9.52, …\n\n# looking at data frame similarities / differences\ncolnames_df1 <- colnames(train_raw)\ncolnames_df2 <- colnames(test_raw)\n  # common columns\ncommon_cols <- intersect(colnames_df1, colnames_df2)\ncommon_cols\n\n [1] \"id\"                \"lat_dec\"           \"lon_dec\"          \n [4] \"no2u_m\"            \"no3u_m\"            \"nh3u_m\"           \n [7] \"r_temp\"            \"r_depth\"           \"r_sal\"            \n[10] \"r_dynht\"           \"r_nuts\"            \"r_oxy_micromol_kg\"\n[13] \"po4u_m\"            \"si_o3u_m\"          \"salinity1\"        \n[16] \"temperature_deg_c\"\n\n  # looking for different columns between the training and testing data\nunique_cols_df1 <- setdiff(colnames_df1, common_cols)\nunique_cols_df1\n\n[1] \"x13\"   \"ta1_x\" \"dic\"  \n\nunique_cols_df2 <- setdiff(colnames_df2, common_cols)\nunique_cols_df2\n\n[1] \"ta1\"\n\n# adjusting training data per column naming differences\ntrain_data <- train_raw |> \n  select(-x13) |> # removing since all NULL\n  rename(ta1 = ta1_x) # changing the name to match the test_raw naming\n\n\n\nData Exploration for Machine Learning Algorithm Selection\nBelow I create a full data set with both the training and testing data. I then explore data distributions for each of the features since this will inform the machine learning algorithm I select for ocean acidification prediction.\n\n# adding all the data together to plot data distributions for each variable. This is to check variance and outliers to inform which ML algorithm to use\nfull_data <- train_data |> \n  select(-dic) |> # removing the dic variable we're predicting since the test data doesn't have this variable and i don't want to look at the data distribution in case leaving out the test data introduces bias\n  rbind(test_raw)\n\n# histograms to explore data distributions and look at variance/outliers\nggplot(data = full_data, aes(x = no2u_m)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = full_data, aes(x = no3u_m)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = full_data, aes(x = nh3u_m)) +\n  geom_histogram() # potential outliers here\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = full_data, aes(x = r_temp)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = full_data, aes(x = r_depth)) +\n  geom_histogram() # potential outliers here:\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = full_data, aes(x = r_sal)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = full_data, aes(x = r_dynht)) +\n  geom_histogram() # potential outliers here\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = full_data, aes(x = r_nuts)) +\n  geom_histogram() # potential outliers here\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nXGBoost Model\nAfter plotting the data distributions, I see a lot of outliers. I also know that I can’t use a classification algorithm since my prediction variable is continuous. I choose to build an extreme gradient boosted treet (XGBoost) machine learning algorithm to build my ocean acidification prediction model. I use this algorithm since gradient boosted models, if properly tuned, can be the most flexible and accurate predictive models. I choose an extreme gradient boosted model since it can handle outliers well and has many hyperparameters that reduce overfitting.\n\nRecipe\n\n# using the xgboost model since this is a powerful ML algorithm that can handle outliers. Code is adapted from: https://bradleyboehmke.github.io/HOML/gbm.html\n\n  # recipe\nxgb_prep <- recipe(dic ~ ., data = train_data) %>% # using all features for predicting dic (ocean acidification)\n  prep(training = train_data, retain = TRUE) %>%\n  juice()\n\n  # separating data used for predictions from the predicted values\nX <- as.matrix(xgb_prep[setdiff(names(xgb_prep), \"dic\")])\nY <- xgb_prep$dic\n\n\n\nHyperparameter Tuning and Model Fit Assessment\n\n# set seed for reproducibility \nset.seed(123)\n\n# creating a hyperparameter grid for all xgboost hyperparameters\nhyper_grid <- expand.grid(\n  eta = 0.01,\n  max_depth = 3, \n  min_child_weight = 3,\n  subsample = 0.5, \n  colsample_bytree = 0.5,\n  gamma = c(0, 1, 10, 100, 1000),\n  lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),\n  alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),\n  rmse = 0,          # a place to dump RMSE results\n  trees = 0          # a place to dump required number of trees\n)\n\n# loop to search through the grid and apply hyperparameters to all 10 cv folds\nfor(i in seq_len(nrow(hyper_grid))) {\n  set.seed(123)\n  m <- xgb.cv(\n    data = X,\n    label = Y,\n    nrounds = 4000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 0,\n    params = list( \n      eta = hyper_grid$eta[i], \n      max_depth = hyper_grid$max_depth[i],\n      min_child_weight = hyper_grid$min_child_weight[i],\n      subsample = hyper_grid$subsample[i],\n      colsample_bytree = hyper_grid$colsample_bytree[i],\n      gamma = hyper_grid$gamma[i], \n      lambda = hyper_grid$lambda[i], \n      alpha = hyper_grid$alpha[i]\n    ) \n  )\n  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)\n  hyper_grid$trees[i] <- m$best_iteration\n}\n\n# store results where rmse is greater than 0\nrmse_hp_results <- hyper_grid %>%\n  filter(rmse > 0) %>%\n  arrange(rmse)\n\n# store the best hyperparameters\nbest_eta <- rmse_hp_results$eta[1]\nbest_md <- rmse_hp_results$max_depth[1]\nbest_cw <- rmse_hp_results$min_child_weight[1]\nbest_ss <- rmse_hp_results$subsample[1]\nbest_csbt <- rmse_hp_results$colsample_bytree[1]\nbest_g <- rmse_hp_results$gamma[1]\nbest_l <- rmse_hp_results$lambda[1]\nbest_a <- rmse_hp_results$alpha[1]\n\n# beep when process is done since this process can take hours\nbeep()\n\n\n\nTraining the Training Data Based on Optimal Hyperparameters\n\n# optimal parameter list\nparams <- list(\n  eta = best_eta,\n  max_depth = best_md,\n  min_child_weight = best_cw,\n  subsample = best_ss,\n  colsample_bytree = best_csbt,\n  gamma = best_g,\n  lambda = best_l,\n  alpha = best_a\n)\n\n# train the model on the training data given these optimal parameters\nxgb.fit.final <- xgboost(\n  params = params,\n  data = X,\n  label = Y,\n  nrounds = 3944,\n  objective = \"reg:squarederror\",\n  verbose = 0\n)\n\n\n\nPredicting the Test Data Set Ocean Acidification for Competition Assessment\n\n# predict on the test data set\nX_test <- as.matrix(test_raw)\ny_pred <- predict(xgb.fit.final, newdata = X_test)\n\ntest_data_with_pred <- cbind(test_raw, DIC = y_pred) |> \n  select(id, DIC) # these are the only columns needed for kaggle competition\n\n\n\nWriting the Final File to a CSV for Competition Assessment\n\n# prompt the user to choose their own file name to write to a csv\nfile_name <- file.choose(new = TRUE)\n\n# write test data predictions to a file\nwrite.csv(test_data_with_pred, file = file_name, row.names = FALSE)\n\n\n\nFinal Thoughts\nBefore the Kaggle Competition closed, each “test data” submission (aka each submission of ocean acidification predictions) were automatically assessed against the real ocean acidification values. Each submission was then given a root mean squared error (RMSE) score based on performance, and everyone could see on the leaderboard how their model performance stacked up compared to others’ model performance. I noticed that many people submitted many times, re-training and fine tuning their models to get better performance.\nHowever, the RMSE score was calculated only using 70% of the submitted data. This meant that the other 30% of the data were left out of RMSE performance metrics prior to the competition end date. As a result, every time a classmate re-trained a model to obtain better performance, they were essentially over fitting to that 70% of the training data. My hunch is that my model won because the 30% of data withheld were not representative of the patterns found in the rest of the data, and I made sure to not introduce bias by re-training and re-tuning my model to serve that 70%."
  },
  {
    "objectID": "posts/suitable-habitats/index.html",
    "href": "posts/suitable-habitats/index.html",
    "title": "Finding Suitable Locations for Marine Aquaculture",
    "section": "",
    "text": "Marine aquaculture holds tremendous promise in addressing the global demand for sustainable protein sources, surpassing conventional land-based meat production. Recent research conducted by Gentry et al. has shed light on the vast potential of marine aquaculture worldwide, highlighting that all current wild-capture fisheries could be replaced by using less than 0.015% of the global ocean area for acquaculture (Gentry et al. 2017).\nIn this post, I embark on a journey to identify the most suitable locations along the west coast of the US for cultivating various marine life. I start by looking at habitat suitable for oysters, and end with a reproducible and reusable workflow workable for any species. My objective is to pinpoint various exclusive economic zones (EEZ)1 that have ideal conditions for marine aquaculture given each desired species ideal sea surface temperature and sea depth ranges.\nA note on study limitations: According to Gentry et al., finding suitable marine aquaculture sites also depends on dissolved oxygen levels, and human-induced constraints such as protected areas, shipping lanes, and oil rigs. According to the study, the lethal limit of dissolved oxygen was not found to be a limiting factor for bivalves (mollusks such as oysters, clams, mussles, an scallops) and dissolved oxygen data is therefore left out of these analyses for simplicity. Human-induced constraints were also left out of this study but should be included for further analysis (Gentry et al. 2017)."
  },
  {
    "objectID": "posts/suitable-habitats/index.html#metadata",
    "href": "posts/suitable-habitats/index.html#metadata",
    "title": "Finding Suitable Locations for Marine Aquaculture",
    "section": "Metadata",
    "text": "Metadata\n\nSea Surface Temperature\nI use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. These data were originally collected from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean I use the General Bathymetric Chart of the Oceans (GEBCO).2\n\n\nExclusive Economic Zones\nI use exclusive economic zonal boundaries from Marineregions.org."
  },
  {
    "objectID": "posts/suitable-habitats/index.html#the-code",
    "href": "posts/suitable-habitats/index.html#the-code",
    "title": "Finding Suitable Locations for Marine Aquaculture",
    "section": "The Code",
    "text": "The Code\n\nLoading the Libraries\n\n# loading necessary libraries\nlibrary(here)\nlibrary(sf)\nlibrary(terra)\nlibrary(raster)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tmap)\n\n\n\nLoading and Cleaning Up the Data\nHere I read in all the spatial data: west coast zone data, yearly sea surface temperature data, and approximate depth data. I then clean the data by (1) checking and aligning coordinate reference systems, map projections, spatial extent, and spatial resolutions (so that our data are correctly aligned in space) and (2) converting data types and units (for dataset compatibility and ease of future calculations).\nYou’ll see that I use the nearest neighbors approach to resample the depth data to match the resolution of the sea surface temperature data, which is of lower resolution. Downsampling like this is a common practice with geospatial datasets since the nearest neighbors approach preserves the original data by assigning the closest pixel in the higher resolution dataset (in this case the closest depth data point) to the newly created lower resolution pixel. I chose this method primarily due to its computational efficiency. Since depth data is continuous, for more accurate analysis something like bilinear or cubic interpolation may be better here.\n\n# set working directory to here\n#setwd(here())\n\n# read in the shapefile of the maritime boundaries of the west coast region\nwc_regions <- st_read(here(\"posts\", \"suitable-habitats\", \"data\", \"wc_regions_clean.shp\"))\n\nReading layer `wc_regions_clean' from data source \n  `/Users/victoriacutler/Documents/MEDS/victoriacutler.github.io/posts/suitable-habitats/data/wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n# read in the yearly sea surface temperature (SST) raster layers\naverage_annual_sst_2008 <- rast(here(\"posts\", \"suitable-habitats\", \"data\", \"average_annual_sst_2008.tif\"))\naverage_annual_sst_2009 <- rast(here(\"posts\", \"suitable-habitats\", \"data\", \"average_annual_sst_2009.tif\"))\naverage_annual_sst_2010 <- rast(here(\"posts\", \"suitable-habitats\", \"data\", \"average_annual_sst_2010.tif\"))\naverage_annual_sst_2011 <- rast(here(\"posts\", \"suitable-habitats\", \"data\", \"average_annual_sst_2011.tif\"))\naverage_annual_sst_2012 <- rast(here(\"posts\", \"suitable-habitats\", \"data\", \"average_annual_sst_2012.tif\"))\n\n# combine rasters into a stack\navg_annual_sst_stack <- stack(c(average_annual_sst_2008, average_annual_sst_2009, average_annual_sst_2010, average_annual_sst_2011, average_annual_sst_2012))\n\n# read in the bathymetry (aka depth) raster\ndepth <- rast(here(\"posts\", \"suitable-habitats\", \"data\", \"depth.tif\"))\n\n# check the coordinate reference systems (CRSs)\ncat(crs(wc_regions))\n\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\ncat(as.character(crs(avg_annual_sst_stack)))\n\n+proj=longlat +ellps=WGS84 +no_defs\n\ncat(crs(depth))\n\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n  # converting the STT stack back to a SpatRaster to use terra functions, like to change the crs\navg_annual_sst_stack <- rast(avg_annual_sst_stack)\n\n  # converting the STT stack SpatRast to the matching crs\ncrs(avg_annual_sst_stack) = \"EPSG:4326\"\n\n# find the mean SST then convert to Celcius\naverage_sst_degC <- mean(avg_annual_sst_stack, na.rm = TRUE) - 273.15\n\n# crop the depth raster to match the extent of the SST raster\ndepth_newextent <- crop(depth, average_sst_degC) # you can see that the extents are now the same\n\n# resample the depth raster such that the resolution is the same as the resolution in the SST raster\ndepth_newres <- resample(x = depth_newextent, y = average_sst_degC, method = \"near\")\n\n# checking that the dimensions, resolutions, extent, and crs match\naverage_sst_degC\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166185, 0.04165702  (x, y)\nextent      : -131.9848, -114.9867, 29.99305, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :   mean \nmin value   :  4.980 \nmax value   : 32.895 \n\ndepth_newres\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166185, 0.04165702  (x, y)\nextent      : -131.9848, -114.9867, 29.99305, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : depth \nmin value   : -5468 \nmax value   :  4218 \n\n  # doing this by checking to see that the two rasters can be stacked\nstack_test <- rast(c(average_sst_degC, depth_newres))\nstack_test # stackable\n\nclass       : SpatRaster \ndimensions  : 480, 408, 2  (nrow, ncol, nlyr)\nresolution  : 0.04166185, 0.04165702  (x, y)\nextent      : -131.9848, -114.9867, 29.99305, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \n\n\n\n\nUsing Reclassification Matrices to Assess Oyster Ecosystem Suitability\nIn this process, I conduct a reclassification of both the sea surface temperature raster and depth raster. Areas with suitable depth or temperature for oyster aquaculture are assigned a value of ‘1’, while unsuitable areas are marked as ‘NA’. By subsequently employing the lapply() function, I perform a multiplication operation on the reclassified rasters. The outcome is a final raster presenting a clear distinction between ‘1s’ indicating suitable habitat and ‘NAs’ representing areas unsuitable for oyster cultivation, providing a comprehensive depiction of overall suitability.\n\n# reclassifying the SST raster for oyster suitability (11-30°C)\n\n  # first creating a reclassification matrix\nrcl_sst <- matrix(c(-Inf, 11, NA,\n                    11, 30, 1,\n                 30, Inf, NA), ncol = 3, byrow = TRUE)\n  # then reclassify the matrix\nsst_suitable <- classify(average_sst_degC, rcl = rcl_sst)\n\n# reclassifying the depth raster for oyster suitability (-70-0)\n\n  # first creating a reclassification matrix\nrcl_depth <- matrix(c(-Inf, -70, NA,\n                    -70, 0, 1,\n                 0, Inf, NA), ncol = 3, byrow = TRUE)\n  # then reclassify the matrix\ndepth_suitable <- classify(depth_newres, rcl = rcl_depth)\n\n# finding locations where depth and sst are suitable for oysters\n  # making a raster multiplication function\nmult_fun = function(rast1, rast2){\n  rast1 * rast2\n}\n\n  # creating the raster that has NAs where both are not suitable, and 1s where depth and sst are suitable\ndepth_and_stt_suitable <- lapp(c(depth_suitable, sst_suitable), fun = mult_fun)\n\n\n\nScaling to Locations of Interest (Ecological Economic Zones)\n\n# data exploration - are these rasters of the same extent/crs etc? yes!\nwc_regions <- wc_regions |> rename(Region = rgn)\ndepth_and_stt_suitable\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166185, 0.04165702  (x, y)\nextent      : -131.9848, -114.9867, 29.99305, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : lyr1 \nmin value   :    1 \nmax value   :    1 \n\n  # first, make wc_regions into a raster\nwc_regions_simp <- wc_regions |> \n  select(\"Region\")\n\nwc_regions_rast <- rasterize(x = wc_regions_simp, y = depth_and_stt_suitable, field = \"Region\")\n\n# finding the suitable cells within each region\n  # first mask the region raster with the suitable cells (since not suitable cells are NA, our region raster will only have values where suitable)\nwc_regions_mask <- mask(wc_regions_rast, depth_and_stt_suitable)\n\n  # lastly, displaying our suitable cells within each region that are suitable\ntm_shape(wc_regions_mask) +\n  tm_raster()\n\n\n\n# finding the area of the suitable grid cells of each region\nsuitable_area_byregion <- zonal(cellSize(wc_regions_mask, unit=\"m\"), wc_regions_mask, fun = \"sum\") |> \n  mutate(area = area / (1000^2)) |> \n  rename(suitable_area_km2 = area)\nsuitable_area_byregion \n\n               Region suitable_area_km2\n1  Central California         4923.1537\n2 Northern California          438.1535\n3              Oregon         1533.0949\n4 Southern California         4096.5404\n5          Washington         3224.7377\n\n# finding the area of the grid cells for each region (even though we have this already reported in the original datafile, we're using the same method we used to calculate our suitable area to make sure percentages are more likely proportional, since there are slight differences in this area approximation method). \ntotal_area_byregion <- zonal(cellSize(wc_regions_rast, unit=\"m\"), wc_regions_rast, fun = \"sum\") |> \n  mutate(area = area / (1000^2)) |> \n  rename(total_area_km2 = area)\ntotal_area_byregion \n\n               Region total_area_km2\n1  Central California      202779.85\n2 Northern California      163715.00\n3              Oregon      179866.42\n4 Southern California      206535.86\n5          Washington       67813.69\n\n# find the percent of suitable area per area per region\npercent_area_df <- inner_join(total_area_byregion, suitable_area_byregion, by = \"Region\") |> mutate(percent_suitable = (suitable_area_km2/total_area_km2) * 100)\n\n# source code: https://stackoverflow.com/questions/68979884/how-to-loop-through-a-dataframe-and-get-value-of-corresponding-column\nfor(i in 1:dim(percent_area_df)[1]) {\n  writeLines((paste0(percent_area_df$Region[i], \" has an area suitable for oysters of \" , round(percent_area_df$suitable_area_km2[i], 2), \"km2 which amounts to an area that is \", round(percent_area_df$percent_suitable[i],2), \"% suitable for oysters.\",\"\\n\")))\n}\n\nCentral California has an area suitable for oysters of 4923.15km2 which amounts to an area that is 2.43% suitable for oysters.\n\nNorthern California has an area suitable for oysters of 438.15km2 which amounts to an area that is 0.27% suitable for oysters.\n\nOregon has an area suitable for oysters of 1533.09km2 which amounts to an area that is 0.85% suitable for oysters.\n\nSouthern California has an area suitable for oysters of 4096.54km2 which amounts to an area that is 1.98% suitable for oysters.\n\nWashington has an area suitable for oysters of 3224.74km2 which amounts to an area that is 4.76% suitable for oysters.\n\n  # test: look at percentages using the given areas to see if theyre similar:\n#percent_area_df_originalareas <- merge(wc_regions, suitable_area_byregion, by = \"Region\") |> mutate(percent_suitable = (suitable_area_km2/area_km2) * 100)\n\n\n\nVisualizing Suitable Ecological Economic Zones\n\n# first joining the the area_df back to the region sf object for plotting\nregion_stats_join <- merge(wc_regions, percent_area_df)\n\n# switching to tmap view mode to add a built in basemap\ntmap_mode(\"view\")\n\n# total suitable area by region\n  # create legend title\nlegend_title_area = expression(\"Suitable Area for the <br> Oyster Species (km<sup>2</sup>)\")\n#legend_title_area = expression(\"Suitable Area for Oysters (km^2)\")\narea_suitable_map <- tm_shape(region_stats_join) +\n  tm_fill(col = \"suitable_area_km2\",\n          title = legend_title_area,\n          style = \"pretty\",\n          palette = \"Blues\") +\n  tm_borders() +\n  tm_legend(legend.outside = TRUE) +\n  #tm_layout(main.title = \"West Coast Regions Suitable for Oysters\",\n            #main.title.size = 1)  +\n  tm_text(\"Region\", size = 0.5) +\n  tm_basemap(\"CartoDB.Positron\") \n\n# percent suitable area by region\npercent_suitable_map <- tm_shape(region_stats_join) +\n  tm_fill(col = \"percent_suitable\",\n          title = \"Percent of Region Suitable <br> for the Oyster Species\",\n          style = \"pretty\",\n          palette = \"Blues\") +\n  tm_borders() +\n  tm_legend(legend.outside = TRUE) +\n # tm_layout(main.title = \"Percent of West Coast Region Suitable for Oysters\",\n           # main.title.size = 1) +\n  tm_text(\"Region\", size = 0.5) +\n  tm_basemap(\"CartoDB.Positron\")\n\ntmap_arrange(area_suitable_map, percent_suitable_map)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Function to Broaden this Workflow\nThis sort of analysis can be applicable for many different applications. Not only can this analysis be used to assess aquaculture suitability for other species, not just the oyster, but it can also be used to assess critical habitat needs for any key marine life that depends on certain sea surface temperature and ocean depth.\nHere I turn my analysis into a function that takes in the marine species of interest, habitat temperature and depth ranges and returns maps of total suitable area and percent suitable area per exclusive economic zone.\nBelow I test my function using data from SeaLifeBase to find suitable habitat for the Giant Rock Scallop, usually found on the west coast of the USA.\n\n# creating a function that takes in depth ranges, temperature ranges, and species name\nsuitability_map_fun = function(shallowest_depth, deepest_depth, min_temp, max_temp, species_name){\n  \n  # code sourced from the \"find suitable locations\" section\nrcl_sst <- matrix(c(-Inf, min_temp, NA,\n                    min_temp, max_temp, 1,\n                 max_temp, Inf, NA), ncol = 3, byrow = TRUE)\nsst_suitable <- classify(average_sst_degC, rcl = rcl_sst)\n\nrcl_depth <- matrix(c(-Inf, -deepest_depth, NA,\n                    -deepest_depth, shallowest_depth, 1,\n                 shallowest_depth, Inf, NA), ncol = 3, byrow = TRUE)\ndepth_suitable <- classify(depth_newres, rcl = rcl_depth)\n\nmult_fun = function(rast1, rast2){\n  rast1 * rast2\n}\ndepth_and_stt_suitable <- lapp(c(depth_suitable, sst_suitable), fun = mult_fun)\n\n  # code sourced from the \"Determine the most suitable EEZ\" section\nwc_regions_simp <- wc_regions |> \n  select(\"Region\")\nwc_regions_rast <- rasterize(x = wc_regions_simp, y = depth_and_stt_suitable, field = \"Region\")\nwc_regions_mask <- mask(wc_regions_rast, depth_and_stt_suitable)\n\nsuitable_area_byregion <- zonal(cellSize(wc_regions_mask, unit=\"m\"), wc_regions_mask, fun = \"sum\") |> \n  mutate(area = area / (1000^2)) |> \n  rename(suitable_area_km2 = area)\n\ntotal_area_byregion <- zonal(cellSize(wc_regions_rast, unit=\"m\"), wc_regions_rast, fun = \"sum\") |> \n  mutate(area = area / (1000^2)) |> \n  rename(total_area_km2 = area)\n\npercent_area_df <- inner_join(total_area_byregion, suitable_area_byregion, by = \"Region\") |> mutate(percent_suitable = (suitable_area_km2/total_area_km2) * 100)\n\n# code sourced from the \"Determine the most suitable EEZ\" section\nregion_stats_join <- merge(wc_regions, percent_area_df)\n\ntmap_mode(\"view\")\n\nlegend_title_area = paste0(\"Suitable Area for the <br>\", species_name, \" Species (km<sup>2</sup>)\")\narea_suitable_map <- tm_shape(region_stats_join) +\n  tm_fill(col = \"suitable_area_km2\",\n          title = legend_title_area,\n          style = \"pretty\",\n          palette = \"Blues\") +\n  tm_borders() +\n  tm_legend(legend.outside = TRUE) +\n  tm_text(\"Region\", size = 0.5) +\n  tm_basemap(\"CartoDB.Positron\") \n\nlegend_title_percent = paste0(\"Percent of Region Suitable <br> for the \", species_name, \" Species\")\npercent_suitable_map <- tm_shape(region_stats_join) +\n  tm_fill(col = \"percent_suitable\",\n          title = legend_title_percent,\n          style = \"pretty\",\n          palette = \"Blues\") +\n  tm_borders() +\n  tm_legend(legend.outside = TRUE) +\n  tm_text(\"Region\", size = 0.5) +\n  tm_basemap(\"CartoDB.Positron\") \n\ntmap_arrange(area_suitable_map, percent_suitable_map)\n\n}\n\n# running the function for the USA giant rock-scallop\n  # source: https://www.sealifebase.ca/summary/Crassadoma-gigantea.html\nsuitability_map_fun(shallowest_depth = 0, deepest_depth = 80, min_temp = 0, max_temp = 22, species_name = \"Giant Rock Scallop\")"
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "ethics",
    "section": "",
    "text": "The Modelers’ Hippocratic Oath\nIn the ever-evolving landscape of technology, the need for updated laws and industry standards is imperative to ensure transparency, accountability, and ethics in how companies handle data. For now, programmers bear a significant portion of the responsibility. The first crucial step lies in raising awareness of ethics in data science and how no mathematical model can ever be a completely accurate representation of the world. However, mathematical models can and do alter the course of our world: whether that be with recidivism models deciding who goes to jail or with falsehoods in micro-targeted political ads to swing voters and influence elections (O’Neil 2017). I therefore re-emphasize the pledge penned by Emanuel Derman and Paul Wilmott during the aftermath of the nefarious model optimizations that ultimately led to the 2008 financial crisis:\n\n\n\n\nhttps://mathbabe.files.wordpress.com/2012/09/modelers_hippocratic_oath.png\n\n\n\n\n\n\n\n\nReferences\n\nO’Neil, Cathy. 2017. Weapons of Math Destruction.\n\nCitationBibTeX citation:@online{cutler2023,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Ethics},\n  date = {2023-07-21},\n  url = {https://victoriacutler.github.io/ethics},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2023. “Ethics.” July 21, 2023. https://victoriacutler.github.io/ethics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Victoria Cutler",
    "section": "",
    "text": "education\nMaster’s in Environmental Data Science | 2023 | Bren School at UCSB\nBachelor’s in Environmental Science | 2018 | Colorado College\n\n\nexperience\nPacific Gas & Electric Company (PG&E) | 2018 - 2022\n\nSenior Strategic Analyst (Energy Efficiency)\nProgram Manager (Community Solar)\nData Analyst (Emergency Operation Center - Public Safety Power Shutoffs)\nRotational Analyst (Energy Efficiency, Demand Response)"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "blog & portfolio",
    "section": "",
    "text": "GEOSPATIAL DATA ANALYSIS\n\n\nMAPPING\n\n\nRASTERS\n\n\nSHAPEFILES\n\n\nRECLASSIFYING\n\n\nMASKING\n\n\nR\n\n\nREMOTE SENSING\n\n\n\nCleaning and reclassifying geospatial vector and raster data to pinpoint optimal locations.\n\n\n\nVictoria Cutler\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPYTHON\n\n\nREMOTE SENSING\n\n\nVEGETATIVE HEALTH\n\n\nNDVI\n\n\nDEFORESTATION\n\n\nLAND COVER CHANGE\n\n\nGOOGLE EARTH ENGINE\n\n\nLANDSAT\n\n\n\nUsing both true- and false- colored satellite imagery.\n\n\n\nVictoria Cutler\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCAL COFI\n\n\nOCEAN ACIDIFICATION\n\n\nMACHINE LEARNING\n\n\nEXTREME GRADIENT BOOSTING\n\n\nR\n\n\n\nA journey through my code to the winning RMSE score.\n\n\n\nVictoria Cutler\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nETHICS\n\n\nAI\n\n\nOPEN SCIENCE\n\n\n\nHow can we construct AI without unintended consequences?\n\n\n\nVictoria Cutler\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROOFTOP SOLAR\n\n\nEQUITY\n\n\nSTATISTICS\n\n\nR\n\n\n\nWho benefits and who doesn’t?\n\n\n\nVictoria Cutler\n\n\nDec 4, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]