[
  {
    "objectID": "posts/2022-12-07-AI/index.html",
    "href": "posts/2022-12-07-AI/index.html",
    "title": "Ethical AI for Just Decision Making",
    "section": "",
    "text": "The judiciary process is set on top of the premise that in-person-judgement in the courtroom helps in accurate decision making. But humans are filled with bias. And what’s more, humans can also be very good at deceiving (Gladwell 2019). Does this mean, then, that we should be using AI to determine what we should do when it comes to the judgement and placement of people? Or, for that matter, for any decisions that may affect humanity?\nUnfortunately, there are countless examples in which AI algorithms create inequities. For one, while at graduate school, Joy Buolamwini sought to create a mirror that projects various “heroes” on her own face. In doing so, she uncovered something alarming. The mirror wouldn’t recognize her own face unless she placed a white mask over it. She then examined the facial recognition technology at Amazon, Google, Microsoft, and IBM and found that error rates within this tech for white men were at less than 1%, yet for black and brown females, this error rate was at a whopping 30%. Buolamwini continued her studies to find that this difference in error rates was largely due to the fact that the data set of imagery was mostly white men (Shacknai 2022). And this is no shock given that performing a quick Google image search of “men”, we largely see white men.\n“When we think about transitioning into the world of tech, the same things that are being marginalized and ignored by the conversations we have around racial inequality in the U.S.—skin tone and colorism—are also being marginalized and ignored in the tech world” - Harvard University Professor Dr. Monk1\nSimilarly, predictive policing2 makes use of past policing data to predict which neighborhoods to police next. While this methodology may be seemingly “unbiased” since it circumvents biased people in the decision making process, the reality is just the opposite. This is due to the fact that policing data is systemically biased and racist. As a result, predictive policing tools can create vicious feedback loops in which neighborhoods that were in the past more frequently policed due to racism and biases are now more policed, which inevitably leads to more arrests/infractions, and therefore more data for predictive policing models to point even more to those very same neighborhoods (Lum and Isaac 2016).\nThis is because “raw data” is not, in fact, raw, in the sense that it is unbiased. It is grounded in the inequities present at the time. For that reason, if we are to use AI to help us with decision making, it is imperative that we re-contextualize our data to recognize the biases at play to help that our model-inputs are no longer biased (Gitelman 2013).\nBut how can we ensure our raw data is re-contextualized and bias-free? One possible mitigation strategy is to utilize datasheets for datasets for any data used in our models (Gebru et al. 2018). In “Datasheets for Datasets” by Timnit Gebru, Gebru brings to light how if we provide a plethora of information about every dataset, we can help avoid unintended consequences in the use of that data. “Datasheets for Datasets” provides a plethora of potential provocative questions to provide about our datasets, such as:\n\n“For what purpose was the dataset created?”\n“Who funded the creation of the dataset?”\n“Does the dataset contain all possible instances or is it a sample (not necessarily random of instances) from a larger set?”\n“Is any information missing from individual instances?”\n“Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?”\n\nWhen we have datasheets comprehensively describing our data, we are able to more accurately identify biases for AI computer model building.\nAnd to that point, our AI models themselves should also come along with datasheets for increased transparency, better model comprehension, and ultimately, better model quality checks. These more transparent AI models are often referred to as “white box” models, which are so transparent such that any outside observer can clearly see how automated AI models arrive at the decision. In this way, it would be much easier to see if the model is making recommendations rooted in bias. “Black box” models do not have this sort of transparency and thus it is nearly impossible to see if the model is using incorrect or unjust assumptions as part of it’s methodology (McNally, n.d.). When we think of how AI models have the ability to morph as the data that the model is continuously trained on comes in, white box models have even more weight. This is because the method that a model was once using and was once understood, may be drastically different from the method used today. For this reason, it is greatly important to perform consistent checks on model assumptions.\nSo - back to the original question: should AI be used for decision making?\nAll in all, AI will be as unjust as the “unjustness” that goes in. But with correct input data and model checks and balances, it is certainly possible to create AI algorithms that are at least less biased than the average human. Algorithms may never be perfect, but that is exactly why it is so important for our models and input data to be transparent and open for diverse sets of stakeholders to scrutinize.\n\n\n\n\n\nReferences\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé, and Kate Crawford. 2018. “Datasheets for Datasets.” https://doi.org/10.48550/ARXIV.1803.09010.\n\n\nGitelman, Lisa, ed. 2013. \"Raw Data\" Is an Oxymoron. The MIT Press. https://doi.org/10.7551/mitpress/9302.001.0001.\n\n\nGladwell, Malcolm. 2019. Talking to Strangers.\n\n\nLum, Kristian, and William Isaac. 2016. “To Predict and Serve?” Significance 13 (5): 14–19. https://doi.org/10.1111/j.1740-9713.2016.00960.x.\n\n\nMcNally, Angelus. n.d. “Creating Trustworthy AI for the Environment: Transparency, Bias, and Beneficial Use.” https://www.scu.edu/environmental-ethics/resources/creating-trustworthy-ai-for-the-environment-transparency-bias-and-beneficial-use/.\n\n\nShacknai, Gabby. 2022. “Beauty in the Eye of the a.i.: How Inherent Racial Bias Has Shaped a.i. And What Brands Are Doing to Address It,” November. https://fortune.com/2022/11/22/tech-forward-everyday-ai-beauty-bias/.\n\nFootnotes\n\n\n(Shacknai 2022)↩︎\nPredictive policing is the use of computer algorithms to predict which neighborhoods to police.↩︎\n\nCitationBibTeX citation:@online{cutler2022,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Ethical {AI} for {Just} {Decision} {Making}},\n  date = {2022-12-07},\n  url = {https://victoriacutler.github.io/posts/2022-10-24-url-title/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2022. “Ethical AI for Just Decision\nMaking.” December 7, 2022. https://victoriacutler.github.io/posts/2022-10-24-url-title/."
  },
  {
    "objectID": "posts/2022-10-24-url-title/index.html",
    "href": "posts/2022-10-24-url-title/index.html",
    "title": "blog post title XXX",
    "section": "",
    "text": "Here is some more text (Lehmann 2007)\nhere i am giong to cite my blog post from last year (Cutler 2022).\n\n\n\n\n\nReferences\n\nCutler, Victoria. 2022. “Blog Post Title XXX.” October 24, 2022. https://victoriacutler.github.io/posts/2022-10-24-url-title/.\n\n\nLehmann, Johannes. 2007. “A Handful of Carbon.” Nature 447 (7141): 143–44. https://doi.org/10.1038/447143a.\n\nFootnotes\n\n\nhere is my footnote 1↩︎\n\nCitationBibTeX citation:@online{cutler2022,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Blog Post Title {XXX}},\n  date = {2022-10-24},\n  url = {https://victoriacutler.github.io/posts/2022-10-24-url-title/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2022. “Blog Post Title XXX.” October 24,\n2022. https://victoriacutler.github.io/posts/2022-10-24-url-title/."
  },
  {
    "objectID": "posts/2022-12-04-22-solar-equity/index.html",
    "href": "posts/2022-12-04-22-solar-equity/index.html",
    "title": "Is Rooftop Solar Equitable?",
    "section": "",
    "text": "Objective\nIn this post, I explore, nationwide, if income is a statistically significant predictor in rooftop solar adoption. Furthermore, I explore this relationship in California, where the California Public Utilities Commission is likely to lower financial incentives to rooftop solar adoption due, in large part, to equity concerns. To quantify how stark this contrast is, I also look to see if there is a significant difference in income relative to solar adoption.\n\n\nMethods\nSince individual household data regarding income and rooftop solar is unavailable for public use, I use:\n\nIncome data on a census tract basis found [here](https://www.ffiec.gov/censusapp.htm) from the Federal Financial Institutions Examination Council (FFIEC) which maintains income-related census data available for download;\nRooftop solar data found [here](https://sunroof.withgoogle.com/data-explorer/featured/1/oklahoma-city) from Google Project Sunroof which uses Google overhead imagery, weather station data, and machine learning algorithms to estimate rooftop solar potential of US buildings down to the census tract level; and\nCalifornia census tract spatial data found [here] (https://catalog.data.gov/dataset/tiger-line-shapefile-2019-state-california-current-census-tract-state-based) for mapping and visualizing at the census tract level.\n\nAfter reading in the data, I cleaned the data, selected only the columns of interest, and created additional columns needed for joining of data and performing analyses. Most notably, I created a variable called pct_qual_with_solar. This variable ranges from 0% to 100% and represents the ratio of the number of buildings with solar with respect to the number of buildings deemed as “qualified” for rooftop solar given the amount of sunlight and roof space. As an example, if census tract A has 10 buildings qualified for rooftop solar, and 5 buildings already have rooftop solar, census tract A would have a pct_qual_with_solar of 50%. I used this variable as a proxy for rooftop solar adoption, since I felt that purely looking at the ratio of of number of buildings with solar with respect to the number of buildings total could lead to bias if income had a relationship with whether or not buildings are qualified for rooftop solar.\n\n\n\nYou can see that\n\n\ncode dropdown\n# Exploration of Distribution/Normality of Solar Data\n\n  # raw solar data distribution for the US\nggplot(data = solardata, aes(x = pct_qual_with_solar)) +\n  geom_histogram() # essentially no left tail, large right skew, good candidate for a log transformation to get data normal\n\n\n\n\n\ncode dropdown\n  # raw solar data distribution for CA\nggplot(data = solardata[solardata$state_name == \"California\",], aes(x = pct_qual_with_solar)) +\n  geom_histogram() # essentially no left tail, large right skew, good candidate for a log transformation to get data normal\n\n\n\n\n\ncode dropdown\n  # the distribution looks more normal when we log it, for the whole US:\nggplot(data = solardata, aes(x = log(pct_qual_with_solar))) +\n  geom_histogram() \n\n\n\n\n\ncode dropdown\n  # the distribution looks more normal when we log it, for california:\nggplot(data = solardata[solardata$state_name == \"California\",], aes(x = log(pct_qual_with_solar))) +\n  geom_histogram() \n\n\n\n\n\n\nchecked to see if OLS assumptions were met\n\n\n\nResults\n\n\n\n\n\nDiscussion\nHi this is my class example blog post\nHere is some more text (lehmann2007?)\nhere i am giong to cite my blog post from last year\n\n\n\n\n\n\nReferences\n\nPenn, Ivan. 2022. “California Regulators Propose Cutting Compensation for Rooftop Solar.” November 10, 2022. https://www.nytimes.com/2022/11/10/business/energy-environment/california-rooftop-solar-net-metering.html.\n\nFootnotes\n\n\nLow income households often lack access to solar since (1) financing options are often needed to cover the high upfront cost of solar (2) if English isn’t a first language, or many jobs ar , there liisn’t enough time to figure out↩︎\n\nCitationBibTeX citation:@online{cutler2022,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Is {Rooftop} {Solar} {Equitable?}},\n  date = {2022-12-08},\n  url = {https://victoriacutler.github.io/posts/2022-12-22-solar-equity/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2022. “Is Rooftop Solar Equitable?”\nDecember 8, 2022. https://victoriacutler.github.io/posts/2022-12-22-solar-equity/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Victoria Cutler",
    "section": "",
    "text": "Education\n\nExperience\nMy website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "ROOFTOP SOLAR\n\n\nEQUITY\n\n\nSTATISTICS\n\n\nR\n\n\n\nthis is a description for XXX\n\n\n\nVictoria Cutler\n\n\nDec 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nETHICS\n\n\nAI\n\n\nOPEN SCIENCE\n\n\n\nHow can we construct AI without unintended consequences?\n\n\n\nVictoria Cutler\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nMEDS\n\n\ntest\n\n\nR\n\n\n\nthis is a description for XXX\n\n\n\nVictoria Cutler\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "hello",
    "section": "",
    "text": "hey"
  }
]