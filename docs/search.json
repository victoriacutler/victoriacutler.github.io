[
  {
    "objectID": "posts/2022-12-07-AI/index.html",
    "href": "posts/2022-12-07-AI/index.html",
    "title": "Ethical AI for Just Decision Making",
    "section": "",
    "text": "The judiciary process is set on top of the premise that in-person-judgement in the courtroom helps in accurate decision making. But humans are filled with bias. And what’s more, humans can also be very good at deceiving (Gladwell 2019). Does this mean, then, that we should be using AI to determine what we should do when it comes to the judgement and placement of people? Or, for that matter, for any decisions that may affect humanity?\nUnfortunately, there are countless examples in which AI algorithms create inequities. For one, while at graduate school, Joy Buolamwini sought to create a mirror that projects various “heroes” on her own face. In doing so, she uncovered something alarming. The mirror wouldn’t recognize her own face unless she placed a white mask over it. She then examined the facial recognition technology at Amazon, Google, Microsoft, and IBM and found that error rates within this tech for white men were at less than 1%, yet for black and brown females, this error rate was at a whopping 30%. Buolamwini continued her studies to find that this difference in error rates was largely due to the fact that the data set of imagery was mostly white men (Shacknai 2022). And this is no shock given that performing a quick Google image search of “men”, we largely see white men.\n“When we think about transitioning into the world of tech, the same things that are being marginalized and ignored by the conversations we have around racial inequality in the U.S.—skin tone and colorism—are also being marginalized and ignored in the tech world” - Harvard University Professor Dr. Monk1\nSimilarly, predictive policing2 makes use of past policing data to predict which neighborhoods to police next. While this methodology may be seemingly “unbiased” since it circumvents biased people in the decision making process, the reality is just the opposite. This is due to the fact that policing data is systemically biased and racist. As a result, predictive policing tools can create vicious feedback loops in which neighborhoods that were in the past more frequently policed due to racism and biases are now more policed, which inevitably leads to more arrests/infractions, and therefore more data for predictive policing models to point even more to those very same neighborhoods (Lum and Isaac 2016).\nThis is because “raw data” is not, in fact, raw, in the sense that it is unbiased. It is grounded in the inequities present at the time. For that reason, if we are to use AI to help us with decision making, it is imperative that we re-contextualize our data to recognize the biases at play to help that our model-inputs are no longer biased (Gitelman 2013).\nBut how can we ensure our raw data is re-contextualized and bias-free? One possible mitigation strategy is to utilize datasheets for datasets for any data used in our models (Gebru et al. 2018). In “Datasheets for Datasets” by Timnit Gebru, Gebru brings to light how if we provide a plethora of information about every dataset, we can help avoid unintended consequences in the use of that data. “Datasheets for Datasets” provides a plethora of potential provocative questions to provide about our datasets, such as:\n\n“For what purpose was the dataset created?”\n“Who funded the creation of the dataset?”\n“Does the dataset contain all possible instances or is it a sample (not necessarily random of instances) from a larger set?”\n“Is any information missing from individual instances?”\n“Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?”\n\nWhen we have datasheets comprehensively describing our data, we are able to more accurately identify biases for AI computer model building.\nAnd to that point, our AI models themselves should also come along with datasheets for increased transparency, better model comprehension, and ultimately, better model quality checks. These more transparent AI models are often referred to as “white box” models, which are so transparent such that any outside observer can clearly see how automated AI models arrive at the decision. In this way, it would be much easier to see if the model is making recommendations rooted in bias. “Black box” models do not have this sort of transparency and thus it is nearly impossible to see if the model is using incorrect or unjust assumptions as part of it’s methodology (McNally, n.d.). When we think of how AI models have the ability to morph as the data that the model is continuously trained on comes in, white box models have even more weight. This is because the method that a model was once using and was once understood, may be drastically different from the method used today. For this reason, it is greatly important to perform consistent checks on model assumptions.\nSo - back to the original question: should AI be used for decision making?\nAll in all, AI will be as unjust as the “unjustness” that goes in. But with correct input data and model checks and balances, it is certainly possible to create AI algorithms that are at least less biased than the average human. Algorithms may never be perfect, but that is exactly why it is so important for our models and input data to be transparent and open for diverse sets of stakeholders to scrutinize.\n\n\n\n\n\nReferences\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé, and Kate Crawford. 2018. “Datasheets for Datasets.” https://doi.org/10.48550/ARXIV.1803.09010.\n\n\nGitelman, Lisa, ed. 2013. \"Raw Data\" Is an Oxymoron. The MIT Press. https://doi.org/10.7551/mitpress/9302.001.0001.\n\n\nGladwell, Malcolm. 2019. Talking to Strangers.\n\n\nLum, Kristian, and William Isaac. 2016. “To Predict and Serve?” Significance 13 (5): 14–19. https://doi.org/10.1111/j.1740-9713.2016.00960.x.\n\n\nMcNally, Angelus. n.d. “Creating Trustworthy AI for the Environment: Transparency, Bias, and Beneficial Use.” https://www.scu.edu/environmental-ethics/resources/creating-trustworthy-ai-for-the-environment-transparency-bias-and-beneficial-use/.\n\n\nShacknai, Gabby. 2022. “Beauty in the Eye of the a.i.: How Inherent Racial Bias Has Shaped a.i. And What Brands Are Doing to Address It,” November. https://fortune.com/2022/11/22/tech-forward-everyday-ai-beauty-bias/.\n\nFootnotes\n\n\n(Shacknai 2022)↩︎\nPredictive policing is the use of computer algorithms to predict which neighborhoods to police.↩︎\n\nCitationBibTeX citation:@online{cutler2022,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Ethical {AI} for {Just} {Decision} {Making}},\n  date = {2022-12-07},\n  url = {https://victoriacutler.github.io/posts/2022-10-24-url-title/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2022. “Ethical AI for Just Decision\nMaking.” December 7, 2022. https://victoriacutler.github.io/posts/2022-10-24-url-title/."
  },
  {
    "objectID": "posts/2022-12-04-22-solar-equity/index.html",
    "href": "posts/2022-12-04-22-solar-equity/index.html",
    "title": "Is Rooftop Solar Equitable?",
    "section": "",
    "text": "Objective\nIn this post, I explore if income is a statistically significant predictor in rooftop solar adoption in California. I also look to see if there is a significant difference in income relative to solar adoption.\n\n\nMethods\nSince individual household data regarding income and rooftop solar is unavailable for public use, I use:\n\nIncome data on a census tract basis found here from the Federal Financial Institutions Examination Council (FFIEC) which maintains income-related census data available for download;\nRooftop solar data found here from Google Project Sunroof which uses Google overhead imagery, weather station data, and machine learning algorithms to estimate rooftop solar potential of US buildings down to the census tract level; and\nCalifornia census tract spatial data found here for mapping and visualizing at the census tract level.\n\nAfter reading in the data, I clean the data, select only the columns of interest, and create additional columns needed for joining of data and performing analyses. Most notably, I created a variable called pct_qual_with_solar. This variable ranges from 0% to 100% and represents the ratio of number of buildings with solar with respect to the number of buildings deemed as “qualified” for rooftop solar2. As an example, if census tract A has 10 buildings qualified for rooftop solar, and 4 buildings in census tract A already have rooftop solar, this census tract would have a pct_qual_with_solar of 40%. I refer to this variable as “solar saturation” since it reveals how saturated a census tract is in terms of rooftop solar adoption. The denominator in this variable is qualified homes as opposed to total buildings, since I feel the latter could lead to bias if income is correlated with locations with census tracts of low rooftop solar qualifications (such as insufficient roof size).\n\n\n\nI first look at the distribution of the solar saturation for California. We see right skew which hints that these data may be a good candidate for log-normalization. However, after taking the log of my data and performing an Ordinary Least Squares (OSL) regression on the log-normalized data, I noticed that the assumptions for OLS were not drastically better met. Additionally, due to the large amount of data we have at 0% solar saturation, to perform a log-normalized OLS regression, this would require either data manipulation, or removing much of the data.\n\n\n\n\n\nIn running my regression, I determine it makes the most sense to account for sunlight when looking at the relationship between census tract income and census tract solar generation. This is because, similar to how solar-qualified buildings may correlate with income, sunlight may also correlate with income, and we want to ensure we are mitigating any omitted variables bias.\nI therefore regressed as follows:\n\\[pctsolarsat_i =\\beta_{0}+\\beta_{1} \\cdot sunlight_i +\\beta_{2} \\cdot \\text income_i+\\varepsilon_i\\]\nWhile I considered that there may be an interaction effect between income and sunlight, I found that variance in solar adoption was not largely explained by this interaction.\nI then performed a hypothesis test to look further at income and rooftop solar adoption, testing the below hypotheses:\n- Null Hypothesis: The true difference in mean solar saturation between lower income and higher income census tracts is equal to 0.\n- Alternative Hypothesis: The true difference in mean solar saturation between lower income and higher income census tracts is not equal to 0.\n\n\nResults and Discussion\n\nLinear Regression\nAfter regressing, we end up with the following model:\n\\[solarsaturation =-0.68 + 0.0000080 \\cdot sunlight + 0.0000384 \\cdot income\\]\n\n\n\n\nTbl 1. Linear Model Results for Below Predictors\n\n \nSolar Saturation (%)\n\n\nPredictors\nEstimates\nConf. Int (95%)\nP-value\n\n\nIntercept\n-0.6756526\n-0.8449613 – -0.5063439\n<0.001\n\n\nMedian Income\n0.0000384\n0.0000369 – 0.0000399\n<0.001\n\n\nAverage Yearly Sunlight Generation (kWh)\n0.0000080\n0.0000055 – 0.0000106\n<0.001\n\n\nObservations\n6379\n\n\nR2 / R2 adjusted\n0.293 / 0.292\n\n\n\n\n\n\nWe see from this model that both median income and sunlight have a positive effect on solar saturation in a census tract, which is expected. Since our coefficient for median income is 0.0000384, for every $1 increase in median yearly income, our model predicts a 0.0000384% increase in solar saturation for that census tract. To put this in more realworld terms, for every $26,042 increase in yearly median census tract income, this model predicts a 1% increase in solar saturation.\nWhat’s interesting is that we see a higher coefficient for median income than sunlight. This means, barring no bias, this model finds that income is a better predictor of solar adoption than sunlight. This provides even more evidence that lower-income households lack access to rooftop solar and the associated financial benefits.\n\n\nHypothesis Testing\nUsing a t-test, we find that the mean solar saturation in higher income census tracts is 4.85%, compared to merely 1.69% in lower income census tracts. Our 95% confidence interval spans from 3.00% to 3.31%, meaning that we are 95% confident that this interval captures the difference in mean solar saturation. These findings are highly significant (p-value < 2.2e-16). We can therefore reject the null hypothesis that there is no difference in mean solar saturation between income groups.\n\n\n\n\n\nSpatial Analysis\nTo visualize this difference in census tract solar adoption, we map solar saturation by census tract for both higher income census tracts and lower income census tracts. We can see, below, visually, that there are much higher saturation levels (represented by the darker blues) in the higher income census tracts. We can also see, evidenced by the amount of grey, that we are missing data for a large number of census tracts. This is due to missing solar data for about 557 of the approximately 8,057 census tracts in California. This could lead to bias in the model, especially if this missing data is not due to random factors.\nAnother take away from this visual is how low solar saturation percentages are overall. This is good and bad news. It means there is still a lot of potential for rooftop solar growth, which is great for clean energy growth. However, this also drives home how important it is to ensure equity in the rooftop solar industry, since these inequities may escalate as solar adoption continues to increase.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpportunities for Improvement\nWhile the findings of this study are statistically significant, we may still be experiencing sources of bias. Below I outline those potential biases, and how further research could aid in better model development:\n\nMissing solar data\nAs mentioned above, the solar data sourced from Google Project Sunroof is missing data from 557 census tracts in California. It is therefore possible that this missing data is creating bias in our model. To ensure that this is not the case, further research could be done to (1) obtain data from these census tracts or (2) inquire to Google Project Sunroof to understand their methodology and gauge whether or not this would lead to an unrepresentative sample.\n\n\n\nInclusion of non-residential buildings\nGoogle Project Sunroof looks at building solar potential for all buildings, not just residential buildings. If non-residential buildings and associated solar adoption is not randomly distributed across census tracts, this could lead to bias in this model. For better modeling, Google Project Sunroof developers could distinguish non-residential and residential buildings.\nMissing variables\nWith any model, there is a chance of omitted variables bias. For further research, I suggest looking into other potential variables that may be correlated with income. Including demographic information such as race could be a good place to start.\n\n\n\nConclusion\nThis statistical analysis provides evidence that low income households have statistically significantly lower levels of rooftop solar adoption. We also see that solar saturation percentages are quite low, which suggests that the rooftop solar industry has the potential to grow much larger. This means that it is of utmost importance that rooftop solar policy is equitable. The California Public Utilities Commission is likely to approve policy to lower financial incentives to rooftop solar adoption due, in large part, to equity concerns. I hope, however, to see policy that encourages rooftop solar but also ensures equity.\n\n\n\n\n\n\nReferences\n\nPenn, Ivan. 2022. “California Regulators Propose Cutting Compensation for Rooftop Solar.” November 10, 2022. https://www.nytimes.com/2022/11/10/business/energy-environment/california-rooftop-solar-net-metering.html.\n\nFootnotes\n\n\nLow income households often lack access to solar since (1) financing options are often needed to cover the high upfront cost of solar (2) if English isn’t a first language, or if multiple jobs are worked, it may be a struggle to find the time to research solar, and (3) renters don’t have the autonomy to install solar panels, and landlords do not have an incentive to help their tenants save on their electric bill.↩︎\nPer Google Project Sunroof criteria which considers amount of sunlight and roof space.↩︎\n\nCitationBibTeX citation:@online{cutler2022,\n  author = {Victoria Cutler},\n  editor = {},\n  title = {Is {Rooftop} {Solar} {Equitable?}},\n  date = {2022-12-04},\n  url = {https://victoriacutler.github.io/posts/2022-12-22-solar-equity/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVictoria Cutler. 2022. “Is Rooftop Solar Equitable?”\nDecember 4, 2022. https://victoriacutler.github.io/posts/2022-12-22-solar-equity/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Victoria Cutler",
    "section": "",
    "text": "Education\n\nExperience\nMy website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "portfolio",
    "section": "",
    "text": "ETHICS\n\n\nAI\n\n\nOPEN SCIENCE\n\n\n\nHow can we construct AI without unintended consequences?\n\n\n\nVictoria Cutler\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROOFTOP SOLAR\n\n\nEQUITY\n\n\nSTATISTICS\n\n\nR\n\n\n\nRooftop solar provides clean energy and cost savings - but are households of all incomes able to reap these benefits? This analysis shows that we see less rooftop solar in…\n\n\n\nVictoria Cutler\n\n\nDec 4, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]