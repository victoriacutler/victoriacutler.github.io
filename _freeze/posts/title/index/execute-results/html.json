{
  "hash": "cbc354950568b6e0824591ad533b32dd",
  "result": {
    "markdown": "---\ntitle: \"Kaggle Competition: X\"\ndescription: \"See below for the winning code for the kaggle competition!\"\nauthor:\n  - name: Victoria Cutler\n    url: https://victoriacutler.github.io # can also add orchid id under here\n    affiliation: MEDS \n    affiliation-url: https://ucsb-meds.github.io\ndate: 2023-03-22\ncategories: [CAL COFI, MACHINE LEARNING, R]\ncitation:\n  url: https://victoriacutler.github.io/posts/title/\nbibliography: references.bib\nformat:\n  html:\n    code-fold: false\n    code-summary: \"code dropdown\"\nimage: \"preview-image.jpg\"\ndraft: false # \"true\" will mean this is a draft post so it wont show up on my site\n---\n\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nhere() starts at /Users/victoriacutler/Documents/MEDS/Courses/EDS221/victoriacutler.github.io\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(recipes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'recipes'\n\nThe following object is masked from 'package:stringr':\n\n    fixed\n\nThe following object is masked from 'package:stats':\n\n    step\n```\n:::\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\nlibrary(beepr)\n```\n:::\n\n\n## Load and Clean Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# storing file paths\ntrain_file_path <- here(\"posts\",\"title\",\"data\", \"train.csv\")\ntest_file_path <- here(\"posts\",\"title\",\"data\", \"test.csv\")\n\n# reading in data\ntrain_raw <- read_csv(file = train_file_path) |> \n  clean_names() |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 1454 Columns: 19\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(18): id, Lat_Dec, Lon_Dec, NO2uM, NO3uM, NH3uM, R_TEMP, R_Depth, R_Sal,... lgl\n(1): ...13\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...13`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,454\nColumns: 19\n$ id                <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ lat_dec           <dbl> 34.38503, 31.41833, 34.38503, 33.48258, 31.41432, 32…\n$ lon_dec           <dbl> -120.6655, -121.9983, -120.6655, -122.5331, -121.997…\n$ no2u_m            <dbl> 0.030, 0.000, 0.180, 0.013, 0.000, 0.080, 0.007, 0.0…\n$ no3u_m            <dbl> 33.80, 34.70, 14.20, 29.67, 33.10, 5.00, 38.76, 0.10…\n$ nh3u_m            <dbl> 0.00, 0.00, 0.00, 0.01, 0.05, 0.14, 0.00, 0.05, 0.03…\n$ r_temp            <dbl> 7.79, 7.12, 11.68, 8.33, 7.53, 13.05, 6.31, 19.28, 7…\n$ r_depth           <dbl> 323, 323, 50, 232, 323, 50, 518, 2, 323, 170, 30, 23…\n$ r_sal             <dbl> 141.2, 140.8, 246.8, 158.5, 143.4, 293.1, 114.6, 403…\n$ r_dynht           <dbl> 0.642, 0.767, 0.144, 0.562, 0.740, 0.176, 0.857, 0.0…\n$ r_nuts            <dbl> 0.00, 0.00, 0.00, 0.01, 0.05, 0.14, 0.00, 0.05, 0.03…\n$ r_oxy_micromol_kg <dbl> 37.40948, 64.81441, 180.29150, 89.62595, 60.03062, 2…\n$ x13               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ po4u_m            <dbl> 2.77, 2.57, 1.29, 2.27, 2.53, 0.75, 3.16, 0.19, 2.63…\n$ si_o3u_m          <dbl> 53.86, 52.50, 13.01, 38.98, 49.28, 5.90, 76.82, 0.17…\n$ ta1_x             <dbl> 2287.45, 2279.10, 2230.80, 2265.85, 2278.49, 2219.87…\n$ salinity1         <dbl> 34.1980, 34.0740, 33.5370, 34.0480, 34.1170, 33.2479…\n$ temperature_deg_c <dbl> 7.82, 7.15, 11.68, 8.36, 7.57, 13.06, 6.36, 19.28, 7…\n$ dic               <dbl> 2270.170, 2254.100, 2111.040, 2223.410, 2252.620, 20…\n```\n:::\n\n```{.r .cell-code}\ntest_raw <- read_csv(file = test_file_path) |> \n  clean_names() |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 485 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): id, Lat_Dec, Lon_Dec, NO2uM, NO3uM, NH3uM, R_TEMP, R_Depth, R_Sal,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 485\nColumns: 17\n$ id                <dbl> 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463…\n$ lat_dec           <dbl> 34.32167, 34.27500, 34.27500, 33.82833, 33.82833, 33…\n$ lon_dec           <dbl> -120.8117, -120.0333, -120.0333, -118.6250, -118.625…\n$ no2u_m            <dbl> 0.02, 0.00, 0.00, 0.00, 0.02, 0.17, 0.00, 0.00, 0.00…\n$ no3u_m            <dbl> 24.0, 25.1, 31.9, 0.0, 19.7, 4.2, 21.5, 26.1, 31.9, …\n$ nh3u_m            <dbl> 0.41, 0.00, 0.00, 0.20, 0.00, 0.10, 0.00, 0.01, 0.00…\n$ r_temp            <dbl> 9.51, 9.84, 6.60, 19.21, 10.65, 12.99, 10.29, 9.50, …\n$ r_depth           <dbl> 101, 102, 514, 1, 100, 19, 99, 169, 319, 523, 101, 5…\n$ r_sal             <dbl> 189.9, 185.2, 124.1, 408.1, 215.5, 284.9, 207.3, 173…\n$ r_dynht           <dbl> 0.258, 0.264, 0.874, 0.004, 0.274, 0.075, 0.266, 0.3…\n$ r_nuts            <dbl> 0.41, 0.00, 0.00, 0.20, 0.00, 0.10, 0.00, 0.01, 0.00…\n$ r_oxy_micromol_kg <dbl> 138.838300, 102.709200, 2.174548, 258.674300, 145.83…\n$ po4u_m            <dbl> 1.85, 2.06, 3.40, 0.27, 1.64, 0.65, 1.77, 2.17, 2.78…\n$ si_o3u_m          <dbl> 25.5, 28.3, 88.1, 2.5, 19.4, 5.4, 21.8, 31.5, 48.6, …\n$ ta1               <dbl> 2244.94, 2253.27, 2316.95, 2240.49, 2238.30, 2224.99…\n$ salinity1         <dbl> 33.8300, 33.9630, 34.2410, 33.4650, 33.7200, 33.3310…\n$ temperature_deg_c <dbl> 9.52, 9.85, 6.65, 19.21, 10.66, 12.99, 10.30, 9.52, …\n```\n:::\n\n```{.r .cell-code}\n# looking at dataframe similarities / differences\ncolnames_df1 <- colnames(train_raw)\ncolnames_df2 <- colnames(test_raw)\n  # common columns\ncommon_cols <- intersect(colnames_df1, colnames_df2)\ncommon_cols\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"id\"                \"lat_dec\"           \"lon_dec\"          \n [4] \"no2u_m\"            \"no3u_m\"            \"nh3u_m\"           \n [7] \"r_temp\"            \"r_depth\"           \"r_sal\"            \n[10] \"r_dynht\"           \"r_nuts\"            \"r_oxy_micromol_kg\"\n[13] \"po4u_m\"            \"si_o3u_m\"          \"salinity1\"        \n[16] \"temperature_deg_c\"\n```\n:::\n\n```{.r .cell-code}\n  # different columns\nunique_cols_df1 <- setdiff(colnames_df1, common_cols)\nunique_cols_df1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"x13\"   \"ta1_x\" \"dic\"  \n```\n:::\n\n```{.r .cell-code}\nunique_cols_df2 <- setdiff(colnames_df2, common_cols)\nunique_cols_df2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ta1\"\n```\n:::\n\n```{.r .cell-code}\n# adjusting training data per column naming differences\ntrain_data <- train_raw |> \n  select(-x13) |> # all NULL\n  rename(ta1 = ta1_x) # to match the test_raw naming\n```\n:::\n\n\n## Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adding all the data together to plot data distributions for variables to check for variance / outliers to inform which ML algorithm to use\nfull_data <- train_data |> \n  select(-dic) |> \n  rbind(test_raw)\n\n# histograms to explore data distributions and look at variance/outliers\nggplot(data = full_data, aes(x = no2u_m)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = no3u_m)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n  # potential outliers here:\nggplot(data = full_data, aes(x = nh3u_m)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = r_temp)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n\n```{.r .cell-code}\n  # potential outliers here:\nggplot(data = full_data, aes(x = r_depth)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-5.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = r_sal)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-6.png){width=672}\n:::\n\n```{.r .cell-code}\n  # potential outliers here:\nggplot(data = full_data, aes(x = r_dynht)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-7.png){width=672}\n:::\n\n```{.r .cell-code}\n  # potential outliers here:\nggplot(data = full_data, aes(x = r_nuts)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-8.png){width=672}\n:::\n:::\n\n\n## XGBoost Model\n\n### Recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using the xgboost model since this is a powerful ML algorithm that can handle outliers. Code is adapted from: https://bradleyboehmke.github.io/HOML/gbm.html\n\n  # recipe\nxgb_prep <- recipe(dic ~ ., data = train_data) %>%\n  prep(training = train_data, retain = TRUE) %>%\n  juice()\n\n  # separating data used for predictions from the predicted values\nX <- as.matrix(xgb_prep[setdiff(names(xgb_prep), \"dic\")])\nY <- xgb_prep$dic\n```\n:::\n\n\n### Hyperparameter Tuning and Model Fit Assessment\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducibility \nset.seed(123)\n\n# creating a hyperparameter grid for all xgboost hyperparameters\nhyper_grid <- expand.grid(\n  eta = 0.01,\n  max_depth = 3, \n  min_child_weight = 3,\n  subsample = 0.5, \n  colsample_bytree = 0.5,\n  gamma = c(0, 1, 10, 100, 1000),\n  lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),\n  alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),\n  rmse = 0,          # a place to dump RMSE results\n  trees = 0          # a place to dump required number of trees\n)\n\n# loop to search through the grid and apply hyperparameters to all 10 cv folds\nfor(i in seq_len(nrow(hyper_grid))) {\n  set.seed(123)\n  m <- xgb.cv(\n    data = X,\n    label = Y,\n    nrounds = 4000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 0,\n    params = list( \n      eta = hyper_grid$eta[i], \n      max_depth = hyper_grid$max_depth[i],\n      min_child_weight = hyper_grid$min_child_weight[i],\n      subsample = hyper_grid$subsample[i],\n      colsample_bytree = hyper_grid$colsample_bytree[i],\n      gamma = hyper_grid$gamma[i], \n      lambda = hyper_grid$lambda[i], \n      alpha = hyper_grid$alpha[i]\n    ) \n  )\n  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)\n  hyper_grid$trees[i] <- m$best_iteration\n}\n\n# store results where rmse is greater than 0\nrmse_hp_results <- hyper_grid %>%\n  filter(rmse > 0) %>%\n  arrange(rmse)\n\n# store the best hyperparameters\nbest_eta <- rmse_hp_results$eta[1]\nbest_md <- rmse_hp_results$max_depth[1]\nbest_cw <- rmse_hp_results$min_child_weight[1]\nbest_ss <- rmse_hp_results$subsample[1]\nbest_csbt <- rmse_hp_results$colsample_bytree[1]\nbest_g <- rmse_hp_results$gamma[1]\nbest_l <- rmse_hp_results$lambda[1]\nbest_a <- rmse_hp_results$alpha[1]\n\n# beep when process is done\nbeep()\n```\n:::\n\n\n### Training the Training Data Based on Optimal Hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# optimal parameter list\nparams <- list(\n  eta = best_eta,\n  max_depth = best_md,\n  min_child_weight = best_cw,\n  subsample = best_ss,\n  colsample_bytree = best_csbt,\n  gamma = best_g,\n  lambda = best_l,\n  alpha = best_a\n)\n\n# train the model on the training data given these optimal parameters\nxgb.fit.final <- xgboost(\n  params = params,\n  data = X,\n  label = Y,\n  nrounds = 3944,\n  objective = \"reg:squarederror\",\n  verbose = 0\n)\n```\n:::\n\n\n### Predicting the Test Data Set DIC\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predict the test data set\nX_test <- as.matrix(test_raw)\ny_pred <- predict(xgb.fit.final, newdata = X_test)\ntest_data_with_pred <- cbind(test_raw, DIC = y_pred) |> \n  select(id, DIC) # these are the only columns needed for kaggle competition\n```\n:::\n\n\n### Writing the Final File to a CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prompt the user to choose their own file name to write to a csv\nfile_name <- file.choose(new = TRUE)\n\n# write test data predictions to a file\nwrite.csv(test_data_with_pred, file = file_name, row.names = FALSE)\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}