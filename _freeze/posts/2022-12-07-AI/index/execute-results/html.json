{
  "hash": "9d36ec9cbb5d1d6d4dcdfaaaf2aa45bc",
  "result": {
    "markdown": "---\ntitle: \"Ethical AI for Decision Making\"\ndescription: \"this is a description for XXX\"\nauthor:\n  - name: Victoria Cutler\n    url: https://victoriacutler.github.io # can also add orchid id under here\n    affiliation: MEDS \n    affiliation-url: https://ucsb-meds.github.io\ndate: 2022-10-24\ncategories: [MEDS, test, R]\ncitation:\n  url: https://victoriacutler.github.io/posts/2022-10-24-url-title/\nbibliography: references.bib\ndraft: false # \"true\" will mean this is a draft post so it wont show up on my site\noutput: \n  html_document:\n    css: custom.css\n---\n\n\nIn \\\"Talking To Strangers\\\" by Malcolm Gladwell, Gladwell remarks that humans are tragically terrible at judging others. Anecdotally, Gladwell references an artificial intelligence (AI) bail sentencing computer program that considers certain facts of the person of deliberation, then returns it\\'s assessment on whether or not the person at hand is likely to commit another crime while out on bail. The facts are striking yet perhaps unsurprising. The AI algorithm better predicts whether arrested persons are likely to reoffend when out on bail, and thus which people should not be given bail. In short, the computer outperforms the judge.\n\nThe judiciary process is set on the premise that scrutinizing someone in person helps assess the situation and how to proceed. But humans are filled with bias. And what\\'s more, humans can also be very good at deceiving [@gladwell2019]. Does this mean, then, that we should be using AI to determine what we should do when it comes to the judgement and placement of people? Or, for that matter, for any decisions that may affect humanity?\n\nUnfortunately, there are countless examples in which AI algorithms create inequities. While at graduate school, Joy Buolamwini sought to create a mirror that projects various \"heroes\" on her own face. In doing so, she uncovered something alarming. The mirror wouldn't recognize her own face unless she placed a white mask over it. She then examined the facial recognition technology at Amazon, Google, Microsoft, and IBM and found that error rates within this tech for white men were at less than 1%, yet for black females, this rate was at a whopping 30%. Buolamwini continued to find that this difference in error rates was largely due to the fact that the data set was mostly white men. \\[facial recognition technology, policing\\].\n\n\n::: {.cell fontsize='30'}\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"“When we think about transitioning into the world of tech, the same things that are being marginalized and ignored by the conversations we have around racial inequality in the U.S.—skin tone and colorism—are also being marginalized and ignored in the tech world” - Harvard University Professor Dr. Monk\"\n```\n:::\n:::\n\n\nThis is because raw data is not, in fact, raw, in the sense that it is unbiased.\n\n\\[but it is possible to make AI unbiases with datasheets for datasets and looking at raw data \\[raw data as an oxymoron\\] and it\\'s important for models to not be a black box. AI will be as unjust as the unjustness that goes in (whether that\\'s due to a lack of diversity, unconscious bias, or raw data that is rooted in injustice)\n\nHi this is my class example blog post\n\nHere is some more text [@lehmann2007]\n\nhere i am giong to cite my blog post from last year [@cutler2022].\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}