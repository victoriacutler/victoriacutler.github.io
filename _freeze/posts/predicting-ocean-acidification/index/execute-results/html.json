{
  "hash": "4df247fdedcdc51284eb97ce46309f11",
  "result": {
    "markdown": "---\ntitle: \"Predicting Ocean Acidification for Ecosystem Management\"\ndescription: \"Ocean acidification, driven primarily by human-induced carbon emissions, already poses a huge problem for our oceans, economy, and well being. For proper ecosystem management, is important to understand which areas of the ocean are at higher risk. This workbook uses machine learning techniques to accurately predict ocean acidification to aid in management.\"\nauthor:\n  - name: Victoria Cutler\n    url: https://victoriacutler.github.io # can also add orchid id under here\n    affiliation: MEDS \n    affiliation-url: https://ucsb-meds.github.io\ndate: 2023-03-22\ncategories: [CAL COFI, OCEAN ACIDIFICATION, MACHINE LEARNING, EXTREME GRADIENT BOOSTING, R]\ncitation:\n  url: https://victoriacutler.github.io/posts/predicting-ocean-acidification/\nbibliography: references.bib\nformat:\n  html:\n    code-fold: false\n    code-summary: \"code dropdown\"\nimage: \"preview-image.png\"\ndraft: false # \"true\" will mean this is a draft post so it wont show up on my site\n---\n\n\n## What is Ocean Acidification?\n\nOur oceans absorb about 30% of the CO~2~ in our atmosphere. So, as CO~2~ emissions increase, CO~2~ in our oceans does, as well. This may not sound like a problem, but CO~2~ quickly breaks down and creates acid that disintegrates things like sea shells and coral reefs that fish, shellfish, and ultimately, humans, depend on. [@NOAA2021] More than 3 billion people around the world rely on food from our oceans[@TNC2021], and ocean acidification poses a huge risk to the availability of the food.\n\n## Kaggle Competition Background\n\nThe California Cooperative Oceanic Fisheries Investigations (CalCOFI for short) studies marine ecosystems across California to help drive sustainable environmental management in the context of climate change.[@CalCOFI2023] In March of 2023, CalCOFI partnered with the Bren School at UC Santa Barbara to search for an effective predictive model for ocean acidification. As part of my Environmental Data Science class, \"Machine Learning in Environmental Science\", us master's students had all entered the Kaggle competition to find the best predictive machine learning model given the data available[^1]. **The prize? CalCOFI swag** **(and helping sustainable ecosystem management).**\n\n[^1]: The data and associated metadata can be found here: https://calcofi.org/data/oceanographic-data/bottle-database/\n\n## The Winning Competition Code\n\nYes, I won. Don't be so shocked. And yes, I got a lot of swag! Please read on, below, for not just the code but also my thought process a long the way.\n\n![See above for the final Kaggle Competition leaderboard results](./leaderboard.png)\n\n### Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggplot2)\nlibrary(recipes)\nlibrary(xgboost)\nlibrary(beepr)\n```\n:::\n\n\n### Load and Clean Data\n\nHere I load in the training and testing data (already pre-split to ensure consistency in competition assessment). I then correct any data quality issues. It turned out that the training and testing data had a couple inconsistencies, so I fixed those up and removed any unnecessary columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# storing file paths\ntrain_file_path <- here(\"posts\",\"predicting-ocean-acidification\",\"data\", \"train.csv\")\ntest_file_path <- here(\"posts\",\"predicting-ocean-acidification\",\"data\", \"test.csv\")\n\n# reading in data\n\n  # raw training data\ntrain_raw <- read_csv(file = train_file_path) |> \n  clean_names() |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,454\nColumns: 19\n$ id                <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ lat_dec           <dbl> 34.38503, 31.41833, 34.38503, 33.48258, 31.41432, 32…\n$ lon_dec           <dbl> -120.6655, -121.9983, -120.6655, -122.5331, -121.997…\n$ no2u_m            <dbl> 0.030, 0.000, 0.180, 0.013, 0.000, 0.080, 0.007, 0.0…\n$ no3u_m            <dbl> 33.80, 34.70, 14.20, 29.67, 33.10, 5.00, 38.76, 0.10…\n$ nh3u_m            <dbl> 0.00, 0.00, 0.00, 0.01, 0.05, 0.14, 0.00, 0.05, 0.03…\n$ r_temp            <dbl> 7.79, 7.12, 11.68, 8.33, 7.53, 13.05, 6.31, 19.28, 7…\n$ r_depth           <dbl> 323, 323, 50, 232, 323, 50, 518, 2, 323, 170, 30, 23…\n$ r_sal             <dbl> 141.2, 140.8, 246.8, 158.5, 143.4, 293.1, 114.6, 403…\n$ r_dynht           <dbl> 0.642, 0.767, 0.144, 0.562, 0.740, 0.176, 0.857, 0.0…\n$ r_nuts            <dbl> 0.00, 0.00, 0.00, 0.01, 0.05, 0.14, 0.00, 0.05, 0.03…\n$ r_oxy_micromol_kg <dbl> 37.40948, 64.81441, 180.29150, 89.62595, 60.03062, 2…\n$ x13               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ po4u_m            <dbl> 2.77, 2.57, 1.29, 2.27, 2.53, 0.75, 3.16, 0.19, 2.63…\n$ si_o3u_m          <dbl> 53.86, 52.50, 13.01, 38.98, 49.28, 5.90, 76.82, 0.17…\n$ ta1_x             <dbl> 2287.45, 2279.10, 2230.80, 2265.85, 2278.49, 2219.87…\n$ salinity1         <dbl> 34.1980, 34.0740, 33.5370, 34.0480, 34.1170, 33.2479…\n$ temperature_deg_c <dbl> 7.82, 7.15, 11.68, 8.36, 7.57, 13.06, 6.36, 19.28, 7…\n$ dic               <dbl> 2270.170, 2254.100, 2111.040, 2223.410, 2252.620, 20…\n```\n:::\n\n```{.r .cell-code}\n  # raw testing data\ntest_raw <- read_csv(file = test_file_path) |> \n  clean_names() |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 485\nColumns: 17\n$ id                <dbl> 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463…\n$ lat_dec           <dbl> 34.32167, 34.27500, 34.27500, 33.82833, 33.82833, 33…\n$ lon_dec           <dbl> -120.8117, -120.0333, -120.0333, -118.6250, -118.625…\n$ no2u_m            <dbl> 0.02, 0.00, 0.00, 0.00, 0.02, 0.17, 0.00, 0.00, 0.00…\n$ no3u_m            <dbl> 24.0, 25.1, 31.9, 0.0, 19.7, 4.2, 21.5, 26.1, 31.9, …\n$ nh3u_m            <dbl> 0.41, 0.00, 0.00, 0.20, 0.00, 0.10, 0.00, 0.01, 0.00…\n$ r_temp            <dbl> 9.51, 9.84, 6.60, 19.21, 10.65, 12.99, 10.29, 9.50, …\n$ r_depth           <dbl> 101, 102, 514, 1, 100, 19, 99, 169, 319, 523, 101, 5…\n$ r_sal             <dbl> 189.9, 185.2, 124.1, 408.1, 215.5, 284.9, 207.3, 173…\n$ r_dynht           <dbl> 0.258, 0.264, 0.874, 0.004, 0.274, 0.075, 0.266, 0.3…\n$ r_nuts            <dbl> 0.41, 0.00, 0.00, 0.20, 0.00, 0.10, 0.00, 0.01, 0.00…\n$ r_oxy_micromol_kg <dbl> 138.838300, 102.709200, 2.174548, 258.674300, 145.83…\n$ po4u_m            <dbl> 1.85, 2.06, 3.40, 0.27, 1.64, 0.65, 1.77, 2.17, 2.78…\n$ si_o3u_m          <dbl> 25.5, 28.3, 88.1, 2.5, 19.4, 5.4, 21.8, 31.5, 48.6, …\n$ ta1               <dbl> 2244.94, 2253.27, 2316.95, 2240.49, 2238.30, 2224.99…\n$ salinity1         <dbl> 33.8300, 33.9630, 34.2410, 33.4650, 33.7200, 33.3310…\n$ temperature_deg_c <dbl> 9.52, 9.85, 6.65, 19.21, 10.66, 12.99, 10.30, 9.52, …\n```\n:::\n\n```{.r .cell-code}\n# looking at data frame similarities / differences\ncolnames_df1 <- colnames(train_raw)\ncolnames_df2 <- colnames(test_raw)\n  # common columns\ncommon_cols <- intersect(colnames_df1, colnames_df2)\ncommon_cols\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"id\"                \"lat_dec\"           \"lon_dec\"          \n [4] \"no2u_m\"            \"no3u_m\"            \"nh3u_m\"           \n [7] \"r_temp\"            \"r_depth\"           \"r_sal\"            \n[10] \"r_dynht\"           \"r_nuts\"            \"r_oxy_micromol_kg\"\n[13] \"po4u_m\"            \"si_o3u_m\"          \"salinity1\"        \n[16] \"temperature_deg_c\"\n```\n:::\n\n```{.r .cell-code}\n  # looking for different columns between the training and testing data\nunique_cols_df1 <- setdiff(colnames_df1, common_cols)\nunique_cols_df1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"x13\"   \"ta1_x\" \"dic\"  \n```\n:::\n\n```{.r .cell-code}\nunique_cols_df2 <- setdiff(colnames_df2, common_cols)\nunique_cols_df2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ta1\"\n```\n:::\n\n```{.r .cell-code}\n# adjusting training data per column naming differences\ntrain_data <- train_raw |> \n  select(-x13) |> # removing since all NULL\n  rename(ta1 = ta1_x) # changing the name to match the test_raw naming\n```\n:::\n\n\n### Data Exploration for Machine Learning Algorithm Selection\n\nBelow I create a full data set with both the training and testing data. I then explore data distributions for each of the features since this will inform the machine learning algorithm I select for ocean acidification prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adding all the data together to plot data distributions for each variable. This is to check variance and outliers to inform which ML algorithm to use\nfull_data <- train_data |> \n  select(-dic) |> # removing the dic variable we're predicting since the test data doesn't have this variable and i don't want to look at the data distribution in case leaving out the test data introduces bias\n  rbind(test_raw)\n\n# histograms to explore data distributions and look at variance/outliers\nggplot(data = full_data, aes(x = no2u_m)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = no3u_m)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = nh3u_m)) +\n  geom_histogram() # potential outliers here\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = r_temp)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = r_depth)) +\n  geom_histogram() # potential outliers here:\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-5.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = r_sal)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-6.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = r_dynht)) +\n  geom_histogram() # potential outliers here\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-7.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = full_data, aes(x = r_nuts)) +\n  geom_histogram() # potential outliers here\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-8.png){width=672}\n:::\n:::\n\n\n### XGBoost Model\n\nAfter plotting the data distributions, I see a lot of outliers. I also know that I can't use a classification algorithm since my prediction variable is continuous. I choose to build an extreme gradient boosted treet (XGBoost) machine learning algorithm to build my ocean acidification prediction model. I use this algorithm since gradient boosted models, if properly tuned, can be the most flexible and accurate predictive models. I choose an extreme gradient boosted model since it can handle outliers well and has many hyperparameters that reduce overfitting.\n\n#### Recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using the xgboost model since this is a powerful ML algorithm that can handle outliers. Code is adapted from: https://bradleyboehmke.github.io/HOML/gbm.html\n\n  # recipe\nxgb_prep <- recipe(dic ~ ., data = train_data) %>% # using all features for predicting dic (ocean acidification)\n  prep(training = train_data, retain = TRUE) %>%\n  juice()\n\n  # separating data used for predictions from the predicted values\nX <- as.matrix(xgb_prep[setdiff(names(xgb_prep), \"dic\")])\nY <- xgb_prep$dic\n```\n:::\n\n\n#### Hyperparameter Tuning and Model Fit Assessment\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducibility \nset.seed(123)\n\n# creating a hyperparameter grid for all xgboost hyperparameters\nhyper_grid <- expand.grid(\n  eta = 0.01,\n  max_depth = 3, \n  min_child_weight = 3,\n  subsample = 0.5, \n  colsample_bytree = 0.5,\n  gamma = c(0, 1, 10, 100, 1000),\n  lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),\n  alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),\n  rmse = 0,          # a place to dump RMSE results\n  trees = 0          # a place to dump required number of trees\n)\n\n# loop to search through the grid and apply hyperparameters to all 10 cv folds\nfor(i in seq_len(nrow(hyper_grid))) {\n  set.seed(123)\n  m <- xgb.cv(\n    data = X,\n    label = Y,\n    nrounds = 4000,\n    objective = \"reg:squarederror\",\n    early_stopping_rounds = 50, \n    nfold = 10,\n    verbose = 0,\n    params = list( \n      eta = hyper_grid$eta[i], \n      max_depth = hyper_grid$max_depth[i],\n      min_child_weight = hyper_grid$min_child_weight[i],\n      subsample = hyper_grid$subsample[i],\n      colsample_bytree = hyper_grid$colsample_bytree[i],\n      gamma = hyper_grid$gamma[i], \n      lambda = hyper_grid$lambda[i], \n      alpha = hyper_grid$alpha[i]\n    ) \n  )\n  hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean)\n  hyper_grid$trees[i] <- m$best_iteration\n}\n\n# store results where rmse is greater than 0\nrmse_hp_results <- hyper_grid %>%\n  filter(rmse > 0) %>%\n  arrange(rmse)\n\n# store the best hyperparameters\nbest_eta <- rmse_hp_results$eta[1]\nbest_md <- rmse_hp_results$max_depth[1]\nbest_cw <- rmse_hp_results$min_child_weight[1]\nbest_ss <- rmse_hp_results$subsample[1]\nbest_csbt <- rmse_hp_results$colsample_bytree[1]\nbest_g <- rmse_hp_results$gamma[1]\nbest_l <- rmse_hp_results$lambda[1]\nbest_a <- rmse_hp_results$alpha[1]\n\n# beep when process is done since this process can take hours\nbeep()\n```\n:::\n\n\n#### Training the Training Data Based on Optimal Hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# optimal parameter list\nparams <- list(\n  eta = best_eta,\n  max_depth = best_md,\n  min_child_weight = best_cw,\n  subsample = best_ss,\n  colsample_bytree = best_csbt,\n  gamma = best_g,\n  lambda = best_l,\n  alpha = best_a\n)\n\n# train the model on the training data given these optimal parameters\nxgb.fit.final <- xgboost(\n  params = params,\n  data = X,\n  label = Y,\n  nrounds = 3944,\n  objective = \"reg:squarederror\",\n  verbose = 0\n)\n```\n:::\n\n\n#### Predicting the Test Data Set Ocean Acidification for Competition Assessment \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predict on the test data set\nX_test <- as.matrix(test_raw)\ny_pred <- predict(xgb.fit.final, newdata = X_test)\n\ntest_data_with_pred <- cbind(test_raw, DIC = y_pred) |> \n  select(id, DIC) # these are the only columns needed for kaggle competition\n```\n:::\n\n\n#### Writing the Final File to a CSV for Competition Assessment\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prompt the user to choose their own file name to write to a csv\nfile_name <- file.choose(new = TRUE)\n\n# write test data predictions to a file\nwrite.csv(test_data_with_pred, file = file_name, row.names = FALSE)\n```\n:::\n\n\n#### Final Thoughts\n\nBefore the Kaggle Competition closed, each \"test data\" submission (aka each submission of ocean acidification predictions) were automatically assessed against the *real* ocean acidification values. Each submission was then given a root mean squared error (RMSE) score based on performance, and everyone could see on the leaderboard how their model performance stacked up compared to others' model performance. I noticed that many people submitted many times, re-training and fine tuning their models to get better performance.\n\nHowever, the RMSE score was calculated only using 70% of the submitted data. This meant that the other 30% of the data were left out of RMSE performance metrics prior to the competition end date. As a result, every time a classmate re-trained a model to obtain better performance, they were essentially over fitting to that 70% of the training data. My hunch is that my model won because the 30% of data withheld were not representative of the patterns found in the rest of the data, and I made sure to not introduce bias by re-training and re-tuning my model to serve that 70%.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}